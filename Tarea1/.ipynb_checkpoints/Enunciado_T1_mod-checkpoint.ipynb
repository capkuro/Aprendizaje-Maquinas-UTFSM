{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-393/578 Máquinas de Aprendizaje - 2019-1 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 1  </H3>\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "**Temas**  \n",
    "* Manipulaciones básicas en pandas\n",
    "* Preprocesamiento y exploración inicial de datos\n",
    "* Regresión Lineal Multivariada\n",
    "* PCA, LDA\n",
    "* Selección de atributos _from scratch_, Métodos Wrapper e Información Mutual\n",
    "* Selección de modelos por _Cross Validation_\n",
    "\n",
    "\n",
    "**Formalidades**  \n",
    "* Equipos de trabajo de 2 personas (*Ambos estudiantes deben estar preparados para presentar la tarea el día de la entrega*)\n",
    "* El entregable debe ser un _Jupyter Notebook_ incluyendo los códigos utilizados, los resultados, los gráficos realizados y comentarios. Debe seguir una estructura similar a un informe (se debe introducir los problemas a trabajar, presentar los resultados y discutirlos). Si lo prefiere puede entregar un _Jupyter Notebook_ por pregunta o un por toda la tarea, con tal de que todos los entregables esten bien identificados y se encuentren en el mismo repositorio de _Github_.\n",
    "* Se debe preparar una presentación del trabajo realizado y sus hallazgos. El presentador será elegido aleatoriamente y deberá apoyarse en el _Jupyter Notebook_ que entregarán. \n",
    "* Formato de entrega: envı́o de link del repositorio en _Github_ ( en caso de ser repositorio privado, invitar como colaborador al usuario de github \"avalderr\") al correo electrónico del ayudante (*<alvaro.valderrama.13@sansano.usm.cl>*), en copia al profesor (*<cvalle@inf.utfsm.cl>*). Especificar el siguiente asunto: [INF393/578-2019 Tarea1]\n",
    "* Fecha de entrega y presentaciones: 26 de Abril. Hora límite de entrega: 23:00. Cualquier _commit_ luego de la hora límite no será evaluado. Se realizará descuento por atrasos en envío del mail. \n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "La tarea se divide en tres partes:\n",
    "\n",
    "[1.](#primero) Aprendizaje con regresión lineal  \n",
    "[2.](#segundo) Análisis de audios como datos brutos  \n",
    "[3.](#tercero) Análisis de emociones en tweets\n",
    "\n",
    "La tarea tiene ejemplos de códigos con los cuales pueden guiarse en gran parte, sin embargo solo son guias y pueden ser creativos al momento de resolver la tarea. Soluciones creativas o elegantes seran valoradas. También en algunas ocaciones se hacen elecciones arbitrarias, ustedes pueden realizar otras elecciones con tal de que haya una pequeña justificación de por qué su elección es mejor o equivalente.\n",
    "Recuerden intercalar su código con comentarios y con celdas _Markdown_ con los comentarios de la pregunta y con cualquier analisis, fórmula o explicación que les parezca relevante para justificar sus procedimientos. \n",
    "Noten que en general cuando se les pide elegir algo o proponer algo no se evaluará mucho la elección en si, en cambio la argumentación detrás de la elección será lo más ponderado.\n",
    "Si algun modelo se demora demasiado en correr en su maquina, no olvide que puede correr _Jupyter Notebooks_ en _Collab_ de Google, esto puede ser relevante para las maquinas más lentas al momento de realizar exploraciones con _K-folds_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"primero\"></a>\n",
    "## 1. Aprendizaje con regresión lineal.\n",
    "\n",
    "La regresión lineal, modelo que busca la mejor forma de combinar linealmente variables para predecir otra variable numérica es una de las herramientas más básicas a nuestra disposición. Sin embargo se puede encontrar muchas aplicaciones a esta aproximación con buenos resultados, y suele ser un buen punto de partida para problemas totalmente desconocidos por su velocidad de implementar y simpleza de interpretar. Además, al igual que muchas de las técnicas de _Machine Learning_, no tiene un ambito de uso definido, es decir podemos aplicar un metodo similar para problemas radicalmente distintos. \n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/cNJQs.png\" style=\"height:50%;\" />\n",
    "\n",
    "En esta primera parte de la tarea, trabajaremos con un _dataset_ de _Kaggle_ llamado \"_Black Friday_\", el cual contiene alrededor de medio millón de registros. Este _dataset_ contiene diversas informaciones sobres los consumidores (edad, ocupación, género, entre otros) y el valor de la compra que realizaron al momento de la medición. Pueden descargar el _dataset_ desde https://www.kaggle.com/mehdidag/black-friday. Este _dataset_ fue pensado para intentar de predecir el valor de compra de un cliente en funcion de sus caracteristicas, sin embargo uno puede realizar distintas regresiones y tratar, por ejemplo, de predecir el rango de edad de un cliente basado en sus otras caracteristicas incluyendo el valor de su compra. En esta primera parte utilizaremos el set de datos para hacer regresión lineal sobre el valor de compra, es decir la variable a predecir es el valor de compra. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " #### 1.a Modulos y carga de datos\n",
    " Comenzaremos cargando las librerias relevantes para el resto de la tarea y cargando los datos a un _DataFrame_ para facilitar su manipulación. Explore los tipos de datos que hemos cargado, los valores que tienen estos datos y como se distribuyen. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  black-friday.zip\n",
      "  inflating: BlackFriday.csv         \n"
     ]
    }
   ],
   "source": [
    "!unzip black-friday.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Product_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>City_Category</th>\n",
       "      <th>Stay_In_Current_City_Years</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Product_Category_1</th>\n",
       "      <th>Product_Category_2</th>\n",
       "      <th>Product_Category_3</th>\n",
       "      <th>Purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000001</td>\n",
       "      <td>P00069042</td>\n",
       "      <td>F</td>\n",
       "      <td>0-17</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000001</td>\n",
       "      <td>P00248942</td>\n",
       "      <td>F</td>\n",
       "      <td>0-17</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000001</td>\n",
       "      <td>P00087842</td>\n",
       "      <td>F</td>\n",
       "      <td>0-17</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000001</td>\n",
       "      <td>P00085442</td>\n",
       "      <td>F</td>\n",
       "      <td>0-17</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000002</td>\n",
       "      <td>P00285442</td>\n",
       "      <td>M</td>\n",
       "      <td>55+</td>\n",
       "      <td>16</td>\n",
       "      <td>C</td>\n",
       "      <td>4+</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID Product_ID Gender   Age  Occupation City_Category  \\\n",
       "0  1000001  P00069042      F  0-17          10             A   \n",
       "1  1000001  P00248942      F  0-17          10             A   \n",
       "2  1000001  P00087842      F  0-17          10             A   \n",
       "3  1000001  P00085442      F  0-17          10             A   \n",
       "4  1000002  P00285442      M   55+          16             C   \n",
       "\n",
       "  Stay_In_Current_City_Years  Marital_Status  Product_Category_1  \\\n",
       "0                          2               0                   3   \n",
       "1                          2               0                   1   \n",
       "2                          2               0                  12   \n",
       "3                          2               0                  12   \n",
       "4                         4+               0                   8   \n",
       "\n",
       "   Product_Category_2  Product_Category_3  Purchase  \n",
       "0                 NaN                 NaN      8370  \n",
       "1                 6.0                14.0     15200  \n",
       "2                 NaN                 NaN      1422  \n",
       "3                14.0                 NaN      1057  \n",
       "4                 NaN                 NaN      7969  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "data = pd.read_csv(\"BlackFriday.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuente los valores N/A de cada columna con el siguiente código y explique brevemente porque en un primer momento preferiremos eliminar la segunda y tercera categoría del producto.\n",
    "\n",
    " Cuantos registros hay de mujeres y de hombres? Cuanto gastan en promedio los distintos rangos de edad? Entregue alguna otra medicion que le parezca interesante y permita familiarizarse con los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User_ID                            0\n",
      "Product_ID                         0\n",
      "Gender                             0\n",
      "Age                                0\n",
      "Occupation                         0\n",
      "City_Category                      0\n",
      "Stay_In_Current_City_Years         0\n",
      "Marital_Status                     0\n",
      "Product_Category_1                 0\n",
      "Product_Category_2            166986\n",
      "Product_Category_3            373299\n",
      "Purchase                           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender\n",
      "F    1666\n",
      "M    4225\n",
      "Name: User_ID, dtype: int64\n",
      "Product_Category_1\n",
      "1     5763\n",
      "2     4283\n",
      "3     3813\n",
      "4     3334\n",
      "5     5746\n",
      "6     4065\n",
      "7     1454\n",
      "8     5656\n",
      "9      404\n",
      "10    2306\n",
      "11    3562\n",
      "12    1556\n",
      "13    2252\n",
      "14     965\n",
      "15    2423\n",
      "16    3102\n",
      "17     419\n",
      "18    1268\n",
      "Name: User_ID, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.groupby(\"Gender\").User_ID.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]\n",
      "[ 2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18. nan]\n",
      "[ 3.  4.  5.  6.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18. nan]\n"
     ]
    }
   ],
   "source": [
    "print(np.sort(data['Product_Category_1'].unique()))\n",
    "print(np.sort(data['Product_Category_2'].unique()))\n",
    "print(np.sort(data['Product_Category_3'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M    405380\n",
      "F    132197\n",
      "Name: Gender, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data['Gender'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age\n",
       "0-17     9020.126878\n",
       "18-25    9235.197575\n",
       "26-35    9314.588970\n",
       "36-45    9401.478758\n",
       "46-50    9284.872277\n",
       "51-55    9620.616620\n",
       "55+      9453.898579\n",
       "Name: Purchase, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('Age').mean()['Purchase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Product_Category_1</th>\n",
       "      <th>Product_Category_2</th>\n",
       "      <th>Product_Category_3</th>\n",
       "      <th>Purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.375770e+05</td>\n",
       "      <td>537577.00000</td>\n",
       "      <td>537577.000000</td>\n",
       "      <td>537577.000000</td>\n",
       "      <td>370591.000000</td>\n",
       "      <td>164278.000000</td>\n",
       "      <td>537577.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.002992e+06</td>\n",
       "      <td>8.08271</td>\n",
       "      <td>0.408797</td>\n",
       "      <td>5.295546</td>\n",
       "      <td>9.842144</td>\n",
       "      <td>12.669840</td>\n",
       "      <td>9333.859853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.714393e+03</td>\n",
       "      <td>6.52412</td>\n",
       "      <td>0.491612</td>\n",
       "      <td>3.750701</td>\n",
       "      <td>5.087259</td>\n",
       "      <td>4.124341</td>\n",
       "      <td>4981.022133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000001e+06</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>185.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.001495e+06</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>5866.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.003031e+06</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>8062.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.004417e+06</td>\n",
       "      <td>14.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>12073.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.006040e+06</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>23961.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            User_ID    Occupation  Marital_Status  Product_Category_1  \\\n",
       "count  5.375770e+05  537577.00000   537577.000000       537577.000000   \n",
       "mean   1.002992e+06       8.08271        0.408797            5.295546   \n",
       "std    1.714393e+03       6.52412        0.491612            3.750701   \n",
       "min    1.000001e+06       0.00000        0.000000            1.000000   \n",
       "25%    1.001495e+06       2.00000        0.000000            1.000000   \n",
       "50%    1.003031e+06       7.00000        0.000000            5.000000   \n",
       "75%    1.004417e+06      14.00000        1.000000            8.000000   \n",
       "max    1.006040e+06      20.00000        1.000000           18.000000   \n",
       "\n",
       "       Product_Category_2  Product_Category_3       Purchase  \n",
       "count       370591.000000       164278.000000  537577.000000  \n",
       "mean             9.842144           12.669840    9333.859853  \n",
       "std              5.087259            4.124341    4981.022133  \n",
       "min              2.000000            3.000000     185.000000  \n",
       "25%              5.000000            9.000000    5866.000000  \n",
       "50%              9.000000           14.000000    8062.000000  \n",
       "75%             15.000000           16.000000   12073.000000  \n",
       "max             18.000000           18.000000   23961.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 1.b Columnas descartadas\n",
    "De los tipos de datos que tenemos cuales resultarán problematicos para realizar regresión lineal? Del dato \"City_Category\", cuantos valores distintos existen? (puede usar el metodo .unique por ejemplo)\n",
    "Porqué a pesar de ser dato numerico las columnas \"Product_Category\" podrían comportarse mal con la regresión lineal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User_ID                         int64\n",
      "Product_ID                     object\n",
      "Gender                         object\n",
      "Age                            object\n",
      "Occupation                      int64\n",
      "City_Category                  object\n",
      "Stay_In_Current_City_Years     object\n",
      "Marital_Status                  int64\n",
      "Product_Category_1              int64\n",
      "Product_Category_2            float64\n",
      "Product_Category_3            float64\n",
      "Purchase                        int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data.dtypes)\n",
    "data = data.drop(columns=['User_ID','Product_ID','Product_Category_1','Product_Category_2','Product_Category_3']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 1.c  Datos Categoricos\n",
    " Transformaremos ahora los datos no numericos a datos numéricos. Para esto transformaremos el género a una variable binaria como muestra el código siguiente. Transforme todos los datos problematicos de manera similar. Discuta la eleccion realizada para \"City_Category\", más adelante haremos otra propuesta. Para los rangos de edad puede utilizar el promedio de ambos valores. Discuta si tiene alguna relevancia la elección del valor numerigo (_e.g._ 1 ó 0 para mujer) para el modelo de regresión lineal. Es así en todos los casos o solo para variables binarias?\n",
    " Por qué el dato \"Occupation\" no podemos utilizarlo directamente a pesar de ser numérico? Que solucion nos da el metodo `get_dummies` de pandas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F' 'M']\n",
      "['0-17' '55+' '26-35' '46-50' '51-55' '36-45' '18-25']\n",
      "['A' 'C' 'B']\n",
      "['2' '4+' '3' '1' '0']\n"
     ]
    }
   ],
   "source": [
    "print(data['Gender'].unique())\n",
    "print(data['Age'].unique())\n",
    "print(data['City_Category'].unique())\n",
    "print(data['Stay_In_Current_City_Years'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_dict = {'F':1,'M':0}\n",
    "age_dict = {'0-17':0,'18-25':1, '26-35':2,'36-45':3,'46-50':4,'51-55':5,'55+':6}\n",
    "siccy_dict = {'0':0,'1':1,'2':2,'3':3,'4+':0}\n",
    "\n",
    "data['Gender'] = data.Gender.apply(lambda x: gender_dict[x])\n",
    "data['Age'] = data.Age.apply(lambda x: age_dict[x])\n",
    "data['Stay_In_Current_City_Years'] = data.Stay_In_Current_City_Years.apply(lambda x: siccy_dict[x])\n",
    "\n",
    "city_dummies = pd.get_dummies(data.City_Category,prefix=\"city\")\n",
    "occupation_dummies = pd.get_dummies(data.Occupation,prefix=\"occupation\")\n",
    "col = list(data.columns)\n",
    "col.remove('City_Category')\n",
    "col.remove('Occupation')\n",
    "data_pre = data[col]\n",
    "data_pre = data_pre.join(city_dummies)\n",
    "data_pre = data_pre.join(occupation_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Stay_In_Current_City_Years</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Purchase</th>\n",
       "      <th>city_A</th>\n",
       "      <th>city_B</th>\n",
       "      <th>city_C</th>\n",
       "      <th>occupation_0</th>\n",
       "      <th>occupation_1</th>\n",
       "      <th>...</th>\n",
       "      <th>occupation_11</th>\n",
       "      <th>occupation_12</th>\n",
       "      <th>occupation_13</th>\n",
       "      <th>occupation_14</th>\n",
       "      <th>occupation_15</th>\n",
       "      <th>occupation_16</th>\n",
       "      <th>occupation_17</th>\n",
       "      <th>occupation_18</th>\n",
       "      <th>occupation_19</th>\n",
       "      <th>occupation_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8370</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>15200</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1422</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1057</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7969</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender  Age  Stay_In_Current_City_Years  Marital_Status  Purchase  city_A  \\\n",
       "0       1    0                           2               0      8370       1   \n",
       "1       1    0                           2               0     15200       1   \n",
       "2       1    0                           2               0      1422       1   \n",
       "3       1    0                           2               0      1057       1   \n",
       "4       0    6                           0               0      7969       0   \n",
       "\n",
       "   city_B  city_C  occupation_0  occupation_1      ...        occupation_11  \\\n",
       "0       0       0             0             0      ...                    0   \n",
       "1       0       0             0             0      ...                    0   \n",
       "2       0       0             0             0      ...                    0   \n",
       "3       0       0             0             0      ...                    0   \n",
       "4       0       1             0             0      ...                    0   \n",
       "\n",
       "   occupation_12  occupation_13  occupation_14  occupation_15  occupation_16  \\\n",
       "0              0              0              0              0              0   \n",
       "1              0              0              0              0              0   \n",
       "2              0              0              0              0              0   \n",
       "3              0              0              0              0              0   \n",
       "4              0              0              0              0              1   \n",
       "\n",
       "   occupation_17  occupation_18  occupation_19  occupation_20  \n",
       "0              0              0              0              0  \n",
       "1              0              0              0              0  \n",
       "2              0              0              0              0  \n",
       "3              0              0              0              0  \n",
       "4              0              0              0              0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pre.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 1.d  Separación de datos\n",
    " Separaremos los datos en _train_, _validation_ y _test_ con 75%, 15% y 10% de los datos respectivamente. Verifique que se cumplen aproximadamente las proporciones. Luego separe el _target_ del resto de las variables que consideraremos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:75.00%, validation:15.00%, test:10.00%\n"
     ]
    }
   ],
   "source": [
    "data_pre_train, data_pre_temp = train_test_split(data_pre, test_size= 0.25)\n",
    "data_pre_val, data_pre_test = train_test_split(data_pre_temp, test_size= 0.4)\n",
    "print(\"train:{:.2f}%, validation:{:.2f}%, test:{:.2f}%\".format(len(data_pre_train)/len(data)*100,len(data_pre_val)/len(data)*100,len(data_pre_test)/len(data)*100))\n",
    "data_x_train = data_pre_train.drop(columns='Purchase')\n",
    "data_y_train = data_pre_train['Purchase']\n",
    "\n",
    "data_x_val = data_pre_val.drop(columns='Purchase')\n",
    "data_y_val = data_pre_val['Purchase']\n",
    "\n",
    "data_x_test = data_pre_test.drop(columns='Purchase')\n",
    "data_y_test = data_pre_test['Purchase']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 1.e  Estandarización\n",
    " Utilizaremos `Standar_Scaler` para estandarizar los datos antes de entrenar. Que ventaja tiene en su opinion el tener un objeto que permite estandarizar (y hacer la transformación inversa) frente a hacer la opreación aritmetica \"manualmente\"? Note el cambio de tipo entre `data_x_train` y `x_train`. Tiene utilidad estandarizar el _target_ para este modelo? Guarde el _target_ estandarizado de todas formas para comparar más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/jupyterhub/lib/python3.6/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype uint8, int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/opt/miniconda/envs/jupyterhub/lib/python3.6/site-packages/ipykernel_launcher.py:5: DataConversionWarning: Data with input dtype uint8, int64 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"\n",
      "/opt/miniconda/envs/jupyterhub/lib/python3.6/site-packages/ipykernel_launcher.py:6: DataConversionWarning: Data with input dtype uint8, int64 were all converted to float64 by StandardScaler.\n",
      "  \n",
      "/opt/miniconda/envs/jupyterhub/lib/python3.6/site-packages/ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype uint8, int64 were all converted to float64 by StandardScaler.\n",
      "  import sys\n",
      "/opt/miniconda/envs/jupyterhub/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/miniconda/envs/jupyterhub/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/miniconda/envs/jupyterhub/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/miniconda/envs/jupyterhub/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "scaler_x.fit(data_x_train)\n",
    "x_tr = scaler_x.transform(data_x_train)\n",
    "x_val = scaler_x.transform(data_x_val)\n",
    "x_test = scaler_x.transform(data_x_test)\n",
    "\n",
    "scaler_y.fit(data_y_train.values.reshape(-1,1))\n",
    "y_tr = scaler_y.transform(data_y_train.values.reshape(-1,1))\n",
    "y_val = scaler_y.transform(data_y_val.values.reshape(-1,1))\n",
    "y_test = scaler_y.transform(data_y_test.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 1.f Primera Regresión\n",
    " Realice una regresión lineal con los datos escalados y no escalados. Compare sus _scores_. Cual es la formula del _score_ que utiliza el regresor (puede buscar en la documentación)? Qué interpretación se le puede dar al _score_? Puede concluir que los datos escalados o no escalados entregan un mejor modelo? Sobre que conjunto de datos (_train_, _test_ o _valitdation_) deberíamos comparar el _score_?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score:0.011154, validation score:0.010437, test score:0.012851\n"
     ]
    }
   ],
   "source": [
    "reg_scaled = LinearRegression()\n",
    "reg_scaled.fit(x_tr,y_tr)\n",
    "train_score = reg_scaled.score(x_tr,y_tr)\n",
    "val_score = reg_scaled.score(x_val,y_val)\n",
    "test_score = reg_scaled.score(x_test,y_test)\n",
    "print(\"train score:{:.6f}, validation score:{:.6f}, test score:{:.6f}\".format(train_score,val_score,test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score:0.011153, validation score:0.010438, test score:0.012842\n"
     ]
    }
   ],
   "source": [
    "reg_unscaled = LinearRegression()\n",
    "reg_unscaled.fit(x_tr,data_y_train.values.reshape(-1,1))\n",
    "train_score = reg_unscaled.score(x_tr,data_y_train.values.reshape(-1,1))\n",
    "val_score = reg_unscaled.score(x_val,data_y_val.values.reshape(-1,1))\n",
    "test_score = reg_unscaled.score(x_test,data_y_test.values.reshape(-1,1))\n",
    "print(\"train score:{:.6f}, validation score:{:.6f}, test score:{:.6f}\".format(train_score,val_score,test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score:0.011163, validation score:0.010409, test score:0.012860\n"
     ]
    }
   ],
   "source": [
    "reg_unscaled = LinearRegression()\n",
    "reg_unscaled.fit(data_x_train,data_y_train.values.reshape(-1,1))\n",
    "train_score = reg_unscaled.score(data_x_train,data_y_train.values.reshape(-1,1))\n",
    "val_score = reg_unscaled.score(data_x_val,data_y_val.values.reshape(-1,1))\n",
    "test_score = reg_unscaled.score(data_x_test,data_y_test.values.reshape(-1,1))\n",
    "print(\"train score:{:.6f}, validation score:{:.6f}, test score:{:.6f}\".format(train_score,val_score,test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 1.g  Tranformaciones simples\n",
    " Grafique como se distribuyen los precios, apoyese en el siguiente código. Recuerde que los gráficos deben ir acompañados de título, nombre de ejes entre otros.\n",
    " Viendo el gráfico, pareciera que una representación lineal funcionará bien para modelar este tipo de datos?\n",
    " \n",
    "Prueba algunas transformaciones simples, como logaritmo o raiz cuadrada y comente los gráficos. \n",
    "\n",
    "Estos gráficos permiten concluir algo sobre el comportamiento de un modelo lineal o solo sirven de caracter exploratorio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-b2fc6c737137>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m plt.plot(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_y_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdata_y_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#np.arange(0,data_y_train.shape[0],1),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#np.linspace(0,((data_y_train.sort_values())).max(),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_y_train' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(\n",
    "    np.arange(0,data_y_train.shape[0],1),\n",
    "    data_y_train.sort_values(),\n",
    "    #np.arange(0,data_y_train.shape[0],1),\n",
    "    #np.linspace(0,((data_y_train.sort_values())).max(),\n",
    "    #            num=data_y_train.shape[0])\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.h 10-_fold_ sobre la potencia\n",
    "Probaremos la utilidad real de realizar una transformación del target del tipo $P \\gets P^{i}$ con $i \\in [0,2]$ haciendo 10-_fold_ variando el valor de $i$ en a lo menos 10 incrementos. Puede basarse en la estrucuta siguiente. Debe elegir un estimador del error de validación de cada modelo (_e.g._ mediana, promedio...) y comentar brevemente por qué eligio tal estimador. \n",
    "Luego grafique como varia el error de validación en función de _i_. \n",
    "Finalmente, con el mejor valor de $i$ encontrado, entrene un regresor lineal con todos los datos de entrenamiento y compare su densempeño con los modelos anteriores.\n",
    "Puede afirmar que alguna de las transformaciones mejore el desempeño del modelo? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = KFold(10)\n",
    "\n",
    "for i in range(1,21):\n",
    "\n",
    "    for train_index, val_index in folds.split(x_tr):\n",
    "        y_i = data_y_train.values**(i/10)\n",
    "        y_tr_i = y_i[train_index]\n",
    "        # . . .\n",
    "        \n",
    "        # entrene el modelo y guarde su score\n",
    "     #  . . ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.i _Q-Q plot_\n",
    "Gráfique el _Quantile-Quantile Plot_ del residuo (error) de validación del merjo modelo que haya encontrado. Qué puede decir respecto al supuesto de normalidad de los residuos en los datos? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.probplot(err ,dist='norm', plot=plt)\n",
    "# puede necesitar hacer err.reshape((1,)) si obtiene errror \"ValueError: all the input array dimensions except for the concatenation axis must match exactly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.j Variables categoricas\n",
    "Entrene ahora el modelo que mejor resultado obtuvo pero esta vez utilizando todos los atributos, transformando las variables categoricas no binarias. Como las variables categóricas no se relacionan linealmente al _target_ (pues no representan un valor numérico), debe transformarlas a variables binarias de igual manera que se realizó con la variable \"Occupation\" en la parte 1.c. \n",
    "Debe cargar de nuevo el DataFrame original pues borramos las columnas de categoría de producto.\n",
    "Discuta por qué podría resultar útil agregar de esta misma manera el dato \"City_Category\" y no como se hizo inicialmente. \n",
    "Discuta los resultados obtenidos en comparación con la parte anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# . . . Cargar datos  nuevamente\n",
    "# . . . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"segundo\"></a>\n",
    "## 2. Selección de Atributos. \n",
    "\n",
    "En esta segunda parte de la tarea estudiaremos un _dataset_ llamado \"House Sales in King County\", un conjunto de datos de ventas de casas, con diversa información sobre las casas vendidas y los precios de venta. Por la naturaleza variada de las casas y las percepciones de las personas que las compran el problema de predecir el valor de una casa a ser vendida resulta interesante pues si bien muchos supuestos razonables se cumplen la mayoría de las veces (una casa más grande suele costar más que una más chica), la percepción de los compradores puede cambiar mucho por caracteristicas dificiles de medir. \n",
    "\n",
    "<img src=\"https://pngimage.net/wp-content/uploads/2018/06/house-for-sale-sign-png-6.png\"  style=\"height:14cm;\"  />\n",
    "\n",
    "Sin embargo, resulta facil imaginar que algunos de los datos resultan más relevantes que otros al momento de predecir, por esto haremos selección de atributos en esta parte de la pregunta. Este _dataset_ es mucho más limpio que lo usual, por lo cual la exploración inicial sera bastante corta. Luego de explorar los datos deberan realizar una exploración de los distintos atributos para evaluar que tan relevantes son en la predicción del resultado. Puede descargar el _dataset_ desde https://www.kaggle.com/harlfoxem/housesalesprediction donde también encontrará una descripción de las distintas columnas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.a Carga de datos \n",
    "\n",
    "Cargue los datos a un DataFrame. Transformaremos la columna \"date\" como indica el siguiente código. Que transformación realiza este código? Elimine las columnas que le parezca pertinente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  housesalesprediction.zip\n",
      "  inflating: kc_house_data.csv       \n"
     ]
    }
   ],
   "source": [
    "!unzip housesalesprediction.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  sqft_living  \\\n",
       "0  7129300520  20141013T000000  221900.0         3       1.00         1180   \n",
       "1  6414100192  20141209T000000  538000.0         3       2.25         2570   \n",
       "2  5631500400  20150225T000000  180000.0         2       1.00          770   \n",
       "3  2487200875  20141209T000000  604000.0         4       3.00         1960   \n",
       "4  1954400510  20150218T000000  510000.0         3       2.00         1680   \n",
       "\n",
       "   sqft_lot  floors  waterfront  view     ...      grade  sqft_above  \\\n",
       "0      5650     1.0           0     0     ...          7        1180   \n",
       "1      7242     2.0           0     0     ...          7        2170   \n",
       "2     10000     1.0           0     0     ...          6         770   \n",
       "3      5000     1.0           0     0     ...          7        1050   \n",
       "4      8080     1.0           0     0     ...          8        1680   \n",
       "\n",
       "   sqft_basement  yr_built  yr_renovated  zipcode      lat     long  \\\n",
       "0              0      1955             0    98178  47.5112 -122.257   \n",
       "1            400      1951          1991    98125  47.7210 -122.319   \n",
       "2              0      1933             0    98028  47.7379 -122.233   \n",
       "3            910      1965             0    98136  47.5208 -122.393   \n",
       "4              0      1987             0    98074  47.6168 -122.045   \n",
       "\n",
       "   sqft_living15  sqft_lot15  \n",
       "0           1340        5650  \n",
       "1           1690        7639  \n",
       "2           2720        8062  \n",
       "3           1360        5000  \n",
       "4           1800        7503  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"kc_house_data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_num(date):\n",
    "    nums = list(date)\n",
    "    res = 0\n",
    "    for i in range(8):\n",
    "        res = res + int(nums[i])*(10**(7-i))\n",
    "    return res\n",
    "data['date'] = data.date.apply(to_num).rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>10544.0</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>13682.5</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>16699.0</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>13682.5</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>16276.0</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id     date     price  bedrooms  bathrooms  sqft_living  sqft_lot  \\\n",
       "0  7129300520  10544.0  221900.0         3       1.00         1180      5650   \n",
       "1  6414100192  13682.5  538000.0         3       2.25         2570      7242   \n",
       "2  5631500400  16699.0  180000.0         2       1.00          770     10000   \n",
       "3  2487200875  13682.5  604000.0         4       3.00         1960      5000   \n",
       "4  1954400510  16276.0  510000.0         3       2.00         1680      8080   \n",
       "\n",
       "   floors  waterfront  view     ...      grade  sqft_above  sqft_basement  \\\n",
       "0     1.0           0     0     ...          7        1180              0   \n",
       "1     2.0           0     0     ...          7        2170            400   \n",
       "2     1.0           0     0     ...          6         770              0   \n",
       "3     1.0           0     0     ...          7        1050            910   \n",
       "4     1.0           0     0     ...          8        1680              0   \n",
       "\n",
       "   yr_built  yr_renovated  zipcode      lat     long  sqft_living15  \\\n",
       "0      1955             0    98178  47.5112 -122.257           1340   \n",
       "1      1951          1991    98125  47.7210 -122.319           1690   \n",
       "2      1933             0    98028  47.7379 -122.233           2720   \n",
       "3      1965             0    98136  47.5208 -122.393           1360   \n",
       "4      1987             0    98074  47.6168 -122.045           1800   \n",
       "\n",
       "   sqft_lot15  \n",
       "0        5650  \n",
       "1        7639  \n",
       "2        8062  \n",
       "3        5000  \n",
       "4        7503  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21613 entries, 0 to 21612\n",
      "Data columns (total 21 columns):\n",
      "id               21613 non-null int64\n",
      "date             21613 non-null float64\n",
      "price            21613 non-null float64\n",
      "bedrooms         21613 non-null int64\n",
      "bathrooms        21613 non-null float64\n",
      "sqft_living      21613 non-null int64\n",
      "sqft_lot         21613 non-null int64\n",
      "floors           21613 non-null float64\n",
      "waterfront       21613 non-null int64\n",
      "view             21613 non-null int64\n",
      "condition        21613 non-null int64\n",
      "grade            21613 non-null int64\n",
      "sqft_above       21613 non-null int64\n",
      "sqft_basement    21613 non-null int64\n",
      "yr_built         21613 non-null int64\n",
      "yr_renovated     21613 non-null int64\n",
      "zipcode          21613 non-null int64\n",
      "lat              21613 non-null float64\n",
      "long             21613 non-null float64\n",
      "sqft_living15    21613 non-null int64\n",
      "sqft_lot15       21613 non-null int64\n",
      "dtypes: float64(6), int64(15)\n",
      "memory usage: 3.5 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.b Preprocesamiento de los datos y visualización\n",
    "Escale los datos de la misma forma que en la pregunta 1. Realice _Scatter plots_ de algunas pares (atributo, precio) que ustedes crean sean más significativos. Comente.\n",
    "\n",
    "En este caso es recomendable estandarizar \"a mano\", pues mantener la estructura de DataFrame facilitará el ítem siguiente. En todo caso, puede elegir la opción que prefiera. \n",
    "\n",
    "Si la curiosidad, imaginación y tiempo le alcanza pruebe también realizando algunas transformaciones simples (logaritmica, cuadrática, _etc._)\n",
    "\n",
    "[//]: <> ( https://vignette.wikia.nocookie.net/inciclopedia/images/6/6a/Bob_esponja_caja_de_imaginacion.JPG/revision/latest?cb=20081122000821 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f66ccdf8ba8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x648 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# scaler\n",
    "\n",
    "plt.figure(figsize=(18,9))\n",
    "plt.scatter(data.sqft_above,data.price)\n",
    "# titles and stuff (:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.c Selección de Atributos _by hand_\n",
    "En esta parte comensaremos entrenando un modelo sin ningun atributo y luego iremos agregando 1 a 1 el atributo que más mejora los resultados de la predicción. En cada paso validaremos qué atributo es el mejor utilizando 5-_fold_ como aprendió en la pregunta anterior. \n",
    "Para el modelo sin ningun atributo utilizaremos una predicción constante igual al promedio de los datos de entrenamiento. Luego, en cada iteración, evaluaremos cada uno de los atributos restantes, viendo cual reduce más el error. El error que utilizaremos para esta pregunta sera el MSE o _Mean Square Error_. Luego de elegido un atributo, lo agregamos a la lista de atributos a utilizar y continuamos iterando hasta agotar los atributos restantes. \n",
    "Guarde los errores promedio a medida va agregando atributos al modelo y guarde la lista de los atributos que fue agregando, luego grafíque el error promedio en función del nuemero de atributos seleccionado.\n",
    "Cual es el modelo que entrenga un menor error de validación (cual es su lista de atributos)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# esta estructura de código es solo una guía y presupone que x_tr es un DataFrame\n",
    "restantes = list(x_tr.columns)\n",
    "actuales = []\n",
    "error_actual = ((y_tr-y_tr.mean())**2).mean()\n",
    "while restantes:\n",
    "    for atributo_candidato in restantes:\n",
    "        # crear nuevo modelo\n",
    "        # hacer 5- fold \n",
    "            model.fit(x_tr[actuales+[atributo_candidato]], y_tr)\n",
    "            #calcular MSE \n",
    "        # promediar MSE y guardar\n",
    "    # evaluar cual se queda y quitarlo de restantes\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.d Validar\n",
    "Utilizando la lista de atributos que calculó en la pregunta anterior, realice un gráfico de como varía el error de entrenamiento (error sobre `x_tr`) y de validación (sobre `x_val`) a medida va agregando los atributos. Comente como sus hallazgos se relacionan con lo aprendido teóricamente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.e Mutual information\n",
    "Calcule la Información Mutual para nuestros datos. Note que esta función ya esta implementada.\n",
    "Que información puede obtener de estos resultados? Concuerdan los datos encontrados con lo encontrado en la pregunta anterior? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "MI = list(zip(mutual_info_regression(x_tr, y_tr),x_tr.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.f Más variables\n",
    "Repita lo realizado en la pregunta 2.c (incluyendo gráficos) pero agregando la variable categorica `zipcode` utilizando nuevamente `pd.get_dummies`. Esta vez puede optar por no realizar _K-fold_ si no entrenar una sola vez cada modelo pues podría resultar demasiado demandante en tiempo utilizar _K-fold_, aunque siempre es valorado. \n",
    "Qué modelo se comporta mejor y por qué cree que sucede esto? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tercero\"></a>\n",
    "## 3. Clasificación. \n",
    "\n",
    "Para esta última parte de la tarea, utilizaremos un conjunto de datos de mediciones al corazón en pacientes con o sin cierta condicion cardiaca. El _dataset_ puede descargarse desde https://www.kaggle.com/shayanfazeli/heartbeat#ptbdb_abnormal.csv, utilizaremos en particular los archivos `ptbdb_normal.csv` y `ptbdb_abnormal.csv`. Nuestro objetivo principal será predecir a partir de las mediciones si un paciente tiene o no tiene la condición. Sin embargo también exploraremos técnicas de reducción de dimensionalidad para tratar de preprocesar los datos, pues una rápida exploración de los datos nos muestra que tenemos 188 atributos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.a Carga de datos\n",
    "Cargue los datos, cree la columna `target` y junte ambos DataFrame en uno solo con método `append`. Por que es necesario pasar una lista de id al cargar los datos?\n",
    "\n",
    "Explore rápidamente los datos: estan muy desbalanceadas las clases? Que tan dispares son las magnitudes de las mediciones? Pareciera necesario estandarizar los datos antes de utilizarlos? \n",
    "\n",
    "Separe los datos utilizando `train_test_split`. Qué habria ocurrido por la forma en que se cargaron los datos si utilizamos la opción `shuffle = False`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([ 4046, 10506]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_normal = pd.read_csv(\"ptbdb_normal.csv\",names=[i for i in range(188)],header=None)\n",
    "data_abnormal = pd.read_csv(\"ptbdb_abnormal.csv\",names=[i for i in range(188)],header=None)\n",
    "data_normal['target'] = 0\n",
    "data_abnormal['target'] = 1\n",
    "data = data_normal.append(data_abnormal,ignore_index=True)\n",
    "np.unique(data['target'],return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.b Correlación\n",
    "Calcule la matriz de correlación, note que esto esta implementado en pandas. Representela de alguna forma que le parezca adecuada, por ejemplo como un _heatmap_. \n",
    "Qué observa, comente.  Elimine las columnas que muestra el código. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144666</td>\n",
       "      <td>-0.177616</td>\n",
       "      <td>-0.152790</td>\n",
       "      <td>-0.096287</td>\n",
       "      <td>-0.065342</td>\n",
       "      <td>-0.055155</td>\n",
       "      <td>-0.046589</td>\n",
       "      <td>-0.041210</td>\n",
       "      <td>-0.038315</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.001774</td>\n",
       "      <td>-0.004780</td>\n",
       "      <td>-0.004405</td>\n",
       "      <td>-0.007503</td>\n",
       "      <td>-0.005206</td>\n",
       "      <td>-0.001655</td>\n",
       "      <td>0.001748</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>-0.054520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.144666</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.751248</td>\n",
       "      <td>0.500707</td>\n",
       "      <td>0.305988</td>\n",
       "      <td>0.212290</td>\n",
       "      <td>0.181973</td>\n",
       "      <td>0.167057</td>\n",
       "      <td>0.159117</td>\n",
       "      <td>0.155542</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014121</td>\n",
       "      <td>-0.016809</td>\n",
       "      <td>-0.015669</td>\n",
       "      <td>-0.012065</td>\n",
       "      <td>-0.008725</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>-0.002080</td>\n",
       "      <td>-0.005430</td>\n",
       "      <td>-0.013417</td>\n",
       "      <td>0.032274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.177616</td>\n",
       "      <td>0.751248</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.838487</td>\n",
       "      <td>0.622316</td>\n",
       "      <td>0.472374</td>\n",
       "      <td>0.384922</td>\n",
       "      <td>0.339727</td>\n",
       "      <td>0.316231</td>\n",
       "      <td>0.304816</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008986</td>\n",
       "      <td>-0.009755</td>\n",
       "      <td>-0.002319</td>\n",
       "      <td>0.004868</td>\n",
       "      <td>0.009894</td>\n",
       "      <td>0.002109</td>\n",
       "      <td>0.006442</td>\n",
       "      <td>0.004487</td>\n",
       "      <td>-0.002900</td>\n",
       "      <td>0.227131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.152790</td>\n",
       "      <td>0.500707</td>\n",
       "      <td>0.838487</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.872682</td>\n",
       "      <td>0.688740</td>\n",
       "      <td>0.561711</td>\n",
       "      <td>0.479327</td>\n",
       "      <td>0.443540</td>\n",
       "      <td>0.422033</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010596</td>\n",
       "      <td>-0.009218</td>\n",
       "      <td>-0.000767</td>\n",
       "      <td>0.008452</td>\n",
       "      <td>0.010660</td>\n",
       "      <td>0.003815</td>\n",
       "      <td>0.013094</td>\n",
       "      <td>0.013047</td>\n",
       "      <td>0.013421</td>\n",
       "      <td>0.306713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.096287</td>\n",
       "      <td>0.305988</td>\n",
       "      <td>0.622316</td>\n",
       "      <td>0.872682</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.894447</td>\n",
       "      <td>0.763733</td>\n",
       "      <td>0.661905</td>\n",
       "      <td>0.602007</td>\n",
       "      <td>0.568914</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010476</td>\n",
       "      <td>0.013435</td>\n",
       "      <td>0.014569</td>\n",
       "      <td>0.025945</td>\n",
       "      <td>0.027586</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.003762</td>\n",
       "      <td>0.006255</td>\n",
       "      <td>0.011019</td>\n",
       "      <td>0.337507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.065342</td>\n",
       "      <td>0.212290</td>\n",
       "      <td>0.472374</td>\n",
       "      <td>0.688740</td>\n",
       "      <td>0.894447</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.939087</td>\n",
       "      <td>0.851245</td>\n",
       "      <td>0.786819</td>\n",
       "      <td>0.741474</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021815</td>\n",
       "      <td>0.025169</td>\n",
       "      <td>0.017714</td>\n",
       "      <td>0.030095</td>\n",
       "      <td>0.031137</td>\n",
       "      <td>0.006153</td>\n",
       "      <td>-0.005582</td>\n",
       "      <td>-0.001191</td>\n",
       "      <td>0.002726</td>\n",
       "      <td>0.276126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.055155</td>\n",
       "      <td>0.181973</td>\n",
       "      <td>0.384922</td>\n",
       "      <td>0.561711</td>\n",
       "      <td>0.763733</td>\n",
       "      <td>0.939087</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.956465</td>\n",
       "      <td>0.908970</td>\n",
       "      <td>0.871848</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024732</td>\n",
       "      <td>0.028833</td>\n",
       "      <td>0.018541</td>\n",
       "      <td>0.030198</td>\n",
       "      <td>0.030683</td>\n",
       "      <td>0.007837</td>\n",
       "      <td>-0.009596</td>\n",
       "      <td>-0.005762</td>\n",
       "      <td>-0.002896</td>\n",
       "      <td>0.213227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.046589</td>\n",
       "      <td>0.167057</td>\n",
       "      <td>0.339727</td>\n",
       "      <td>0.479327</td>\n",
       "      <td>0.661905</td>\n",
       "      <td>0.851245</td>\n",
       "      <td>0.956465</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.976515</td>\n",
       "      <td>0.952481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022336</td>\n",
       "      <td>0.027147</td>\n",
       "      <td>0.015354</td>\n",
       "      <td>0.027008</td>\n",
       "      <td>0.027375</td>\n",
       "      <td>0.007368</td>\n",
       "      <td>-0.012403</td>\n",
       "      <td>-0.009182</td>\n",
       "      <td>-0.007000</td>\n",
       "      <td>0.206502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.041210</td>\n",
       "      <td>0.159117</td>\n",
       "      <td>0.316231</td>\n",
       "      <td>0.443540</td>\n",
       "      <td>0.602007</td>\n",
       "      <td>0.786819</td>\n",
       "      <td>0.908970</td>\n",
       "      <td>0.976515</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020624</td>\n",
       "      <td>0.025515</td>\n",
       "      <td>0.013352</td>\n",
       "      <td>0.025011</td>\n",
       "      <td>0.025351</td>\n",
       "      <td>0.005706</td>\n",
       "      <td>-0.014867</td>\n",
       "      <td>-0.011476</td>\n",
       "      <td>-0.009371</td>\n",
       "      <td>0.208454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.038315</td>\n",
       "      <td>0.155542</td>\n",
       "      <td>0.304816</td>\n",
       "      <td>0.422033</td>\n",
       "      <td>0.568914</td>\n",
       "      <td>0.741474</td>\n",
       "      <td>0.871848</td>\n",
       "      <td>0.952481</td>\n",
       "      <td>0.984051</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020235</td>\n",
       "      <td>0.025054</td>\n",
       "      <td>0.012868</td>\n",
       "      <td>0.024328</td>\n",
       "      <td>0.024391</td>\n",
       "      <td>0.005428</td>\n",
       "      <td>-0.015574</td>\n",
       "      <td>-0.012136</td>\n",
       "      <td>-0.010147</td>\n",
       "      <td>0.207081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.036839</td>\n",
       "      <td>0.145552</td>\n",
       "      <td>0.290853</td>\n",
       "      <td>0.403009</td>\n",
       "      <td>0.540335</td>\n",
       "      <td>0.713634</td>\n",
       "      <td>0.841678</td>\n",
       "      <td>0.930747</td>\n",
       "      <td>0.970819</td>\n",
       "      <td>0.988453</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020492</td>\n",
       "      <td>0.024999</td>\n",
       "      <td>0.012814</td>\n",
       "      <td>0.024350</td>\n",
       "      <td>0.024494</td>\n",
       "      <td>0.005562</td>\n",
       "      <td>-0.015985</td>\n",
       "      <td>-0.012582</td>\n",
       "      <td>-0.010825</td>\n",
       "      <td>0.205424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.031709</td>\n",
       "      <td>0.143267</td>\n",
       "      <td>0.278851</td>\n",
       "      <td>0.388595</td>\n",
       "      <td>0.525051</td>\n",
       "      <td>0.693748</td>\n",
       "      <td>0.827019</td>\n",
       "      <td>0.911671</td>\n",
       "      <td>0.956062</td>\n",
       "      <td>0.978919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020620</td>\n",
       "      <td>0.024562</td>\n",
       "      <td>0.012498</td>\n",
       "      <td>0.023476</td>\n",
       "      <td>0.023792</td>\n",
       "      <td>0.004970</td>\n",
       "      <td>-0.016622</td>\n",
       "      <td>-0.013275</td>\n",
       "      <td>-0.011175</td>\n",
       "      <td>0.200058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.030020</td>\n",
       "      <td>0.134683</td>\n",
       "      <td>0.269779</td>\n",
       "      <td>0.373314</td>\n",
       "      <td>0.507993</td>\n",
       "      <td>0.678131</td>\n",
       "      <td>0.807888</td>\n",
       "      <td>0.899652</td>\n",
       "      <td>0.941352</td>\n",
       "      <td>0.968868</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021041</td>\n",
       "      <td>0.024721</td>\n",
       "      <td>0.012598</td>\n",
       "      <td>0.023616</td>\n",
       "      <td>0.023584</td>\n",
       "      <td>0.004151</td>\n",
       "      <td>-0.016935</td>\n",
       "      <td>-0.013592</td>\n",
       "      <td>-0.011809</td>\n",
       "      <td>0.197330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.028928</td>\n",
       "      <td>0.133368</td>\n",
       "      <td>0.260469</td>\n",
       "      <td>0.363856</td>\n",
       "      <td>0.493537</td>\n",
       "      <td>0.664019</td>\n",
       "      <td>0.795842</td>\n",
       "      <td>0.886751</td>\n",
       "      <td>0.935541</td>\n",
       "      <td>0.959676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018600</td>\n",
       "      <td>0.022531</td>\n",
       "      <td>0.011268</td>\n",
       "      <td>0.022034</td>\n",
       "      <td>0.022100</td>\n",
       "      <td>0.003690</td>\n",
       "      <td>-0.017140</td>\n",
       "      <td>-0.013808</td>\n",
       "      <td>-0.011621</td>\n",
       "      <td>0.194571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.026765</td>\n",
       "      <td>0.127772</td>\n",
       "      <td>0.252131</td>\n",
       "      <td>0.347797</td>\n",
       "      <td>0.479941</td>\n",
       "      <td>0.646577</td>\n",
       "      <td>0.781524</td>\n",
       "      <td>0.875866</td>\n",
       "      <td>0.921680</td>\n",
       "      <td>0.953206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019154</td>\n",
       "      <td>0.023352</td>\n",
       "      <td>0.011549</td>\n",
       "      <td>0.022185</td>\n",
       "      <td>0.022558</td>\n",
       "      <td>0.004120</td>\n",
       "      <td>-0.017522</td>\n",
       "      <td>-0.014228</td>\n",
       "      <td>-0.012073</td>\n",
       "      <td>0.190590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.026061</td>\n",
       "      <td>0.118841</td>\n",
       "      <td>0.241677</td>\n",
       "      <td>0.333328</td>\n",
       "      <td>0.460753</td>\n",
       "      <td>0.632024</td>\n",
       "      <td>0.763219</td>\n",
       "      <td>0.861346</td>\n",
       "      <td>0.909323</td>\n",
       "      <td>0.936118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018870</td>\n",
       "      <td>0.022965</td>\n",
       "      <td>0.011934</td>\n",
       "      <td>0.022611</td>\n",
       "      <td>0.022803</td>\n",
       "      <td>0.003039</td>\n",
       "      <td>-0.018248</td>\n",
       "      <td>-0.014935</td>\n",
       "      <td>-0.012982</td>\n",
       "      <td>0.187275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.019610</td>\n",
       "      <td>0.115867</td>\n",
       "      <td>0.229069</td>\n",
       "      <td>0.317812</td>\n",
       "      <td>0.445454</td>\n",
       "      <td>0.613594</td>\n",
       "      <td>0.751415</td>\n",
       "      <td>0.846490</td>\n",
       "      <td>0.896696</td>\n",
       "      <td>0.925324</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018589</td>\n",
       "      <td>0.022758</td>\n",
       "      <td>0.011972</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.022416</td>\n",
       "      <td>0.002553</td>\n",
       "      <td>-0.018148</td>\n",
       "      <td>-0.014934</td>\n",
       "      <td>-0.012468</td>\n",
       "      <td>0.179537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.015755</td>\n",
       "      <td>0.103024</td>\n",
       "      <td>0.216203</td>\n",
       "      <td>0.298890</td>\n",
       "      <td>0.426767</td>\n",
       "      <td>0.598883</td>\n",
       "      <td>0.736706</td>\n",
       "      <td>0.840162</td>\n",
       "      <td>0.887786</td>\n",
       "      <td>0.919132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018408</td>\n",
       "      <td>0.022460</td>\n",
       "      <td>0.011437</td>\n",
       "      <td>0.022042</td>\n",
       "      <td>0.022074</td>\n",
       "      <td>0.002529</td>\n",
       "      <td>-0.018745</td>\n",
       "      <td>-0.015619</td>\n",
       "      <td>-0.013056</td>\n",
       "      <td>0.173814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.015344</td>\n",
       "      <td>0.097122</td>\n",
       "      <td>0.202663</td>\n",
       "      <td>0.286897</td>\n",
       "      <td>0.413629</td>\n",
       "      <td>0.589740</td>\n",
       "      <td>0.730408</td>\n",
       "      <td>0.830623</td>\n",
       "      <td>0.885410</td>\n",
       "      <td>0.913458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017291</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>0.010749</td>\n",
       "      <td>0.021282</td>\n",
       "      <td>0.021348</td>\n",
       "      <td>0.001593</td>\n",
       "      <td>-0.019166</td>\n",
       "      <td>-0.016138</td>\n",
       "      <td>-0.013312</td>\n",
       "      <td>0.166952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.009646</td>\n",
       "      <td>0.083423</td>\n",
       "      <td>0.179758</td>\n",
       "      <td>0.253454</td>\n",
       "      <td>0.382197</td>\n",
       "      <td>0.557571</td>\n",
       "      <td>0.702003</td>\n",
       "      <td>0.805509</td>\n",
       "      <td>0.858494</td>\n",
       "      <td>0.893039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017421</td>\n",
       "      <td>0.021651</td>\n",
       "      <td>0.010842</td>\n",
       "      <td>0.021292</td>\n",
       "      <td>0.020912</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>-0.020178</td>\n",
       "      <td>-0.017193</td>\n",
       "      <td>-0.014309</td>\n",
       "      <td>0.154225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.006830</td>\n",
       "      <td>0.059098</td>\n",
       "      <td>0.147493</td>\n",
       "      <td>0.215405</td>\n",
       "      <td>0.340937</td>\n",
       "      <td>0.526901</td>\n",
       "      <td>0.674260</td>\n",
       "      <td>0.783439</td>\n",
       "      <td>0.839609</td>\n",
       "      <td>0.871788</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016782</td>\n",
       "      <td>0.020970</td>\n",
       "      <td>0.010469</td>\n",
       "      <td>0.021113</td>\n",
       "      <td>0.020927</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>-0.020913</td>\n",
       "      <td>-0.017847</td>\n",
       "      <td>-0.014712</td>\n",
       "      <td>0.140680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.001725</td>\n",
       "      <td>0.044249</td>\n",
       "      <td>0.119768</td>\n",
       "      <td>0.182764</td>\n",
       "      <td>0.307545</td>\n",
       "      <td>0.493882</td>\n",
       "      <td>0.651316</td>\n",
       "      <td>0.758381</td>\n",
       "      <td>0.816703</td>\n",
       "      <td>0.851037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017358</td>\n",
       "      <td>0.021906</td>\n",
       "      <td>0.011710</td>\n",
       "      <td>0.022277</td>\n",
       "      <td>0.021932</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>-0.020701</td>\n",
       "      <td>-0.017716</td>\n",
       "      <td>-0.014310</td>\n",
       "      <td>0.122510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.005993</td>\n",
       "      <td>0.033550</td>\n",
       "      <td>0.107875</td>\n",
       "      <td>0.162072</td>\n",
       "      <td>0.289274</td>\n",
       "      <td>0.483738</td>\n",
       "      <td>0.640591</td>\n",
       "      <td>0.755335</td>\n",
       "      <td>0.809926</td>\n",
       "      <td>0.846735</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017582</td>\n",
       "      <td>0.022081</td>\n",
       "      <td>0.011581</td>\n",
       "      <td>0.022328</td>\n",
       "      <td>0.021891</td>\n",
       "      <td>-0.000383</td>\n",
       "      <td>-0.021794</td>\n",
       "      <td>-0.018873</td>\n",
       "      <td>-0.015396</td>\n",
       "      <td>0.108900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.010304</td>\n",
       "      <td>0.039312</td>\n",
       "      <td>0.109502</td>\n",
       "      <td>0.166528</td>\n",
       "      <td>0.292913</td>\n",
       "      <td>0.490359</td>\n",
       "      <td>0.648534</td>\n",
       "      <td>0.757380</td>\n",
       "      <td>0.816135</td>\n",
       "      <td>0.847791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014523</td>\n",
       "      <td>0.019052</td>\n",
       "      <td>0.009495</td>\n",
       "      <td>0.020326</td>\n",
       "      <td>0.019503</td>\n",
       "      <td>-0.002940</td>\n",
       "      <td>-0.023969</td>\n",
       "      <td>-0.021007</td>\n",
       "      <td>-0.017389</td>\n",
       "      <td>0.095446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.011958</td>\n",
       "      <td>0.040149</td>\n",
       "      <td>0.109621</td>\n",
       "      <td>0.157950</td>\n",
       "      <td>0.288612</td>\n",
       "      <td>0.484417</td>\n",
       "      <td>0.643996</td>\n",
       "      <td>0.751993</td>\n",
       "      <td>0.804470</td>\n",
       "      <td>0.840643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013778</td>\n",
       "      <td>0.018455</td>\n",
       "      <td>0.008540</td>\n",
       "      <td>0.019384</td>\n",
       "      <td>0.018258</td>\n",
       "      <td>-0.005007</td>\n",
       "      <td>-0.025427</td>\n",
       "      <td>-0.022516</td>\n",
       "      <td>-0.018662</td>\n",
       "      <td>0.076036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.011408</td>\n",
       "      <td>0.036453</td>\n",
       "      <td>0.103524</td>\n",
       "      <td>0.144557</td>\n",
       "      <td>0.270292</td>\n",
       "      <td>0.474470</td>\n",
       "      <td>0.630004</td>\n",
       "      <td>0.737695</td>\n",
       "      <td>0.790637</td>\n",
       "      <td>0.821950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011876</td>\n",
       "      <td>0.016556</td>\n",
       "      <td>0.007012</td>\n",
       "      <td>0.017811</td>\n",
       "      <td>0.016775</td>\n",
       "      <td>-0.005949</td>\n",
       "      <td>-0.025981</td>\n",
       "      <td>-0.023102</td>\n",
       "      <td>-0.019127</td>\n",
       "      <td>0.054966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.014238</td>\n",
       "      <td>0.032773</td>\n",
       "      <td>0.088128</td>\n",
       "      <td>0.119389</td>\n",
       "      <td>0.242694</td>\n",
       "      <td>0.447163</td>\n",
       "      <td>0.609046</td>\n",
       "      <td>0.711606</td>\n",
       "      <td>0.765740</td>\n",
       "      <td>0.798631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011107</td>\n",
       "      <td>0.016048</td>\n",
       "      <td>0.006552</td>\n",
       "      <td>0.016982</td>\n",
       "      <td>0.015484</td>\n",
       "      <td>-0.008185</td>\n",
       "      <td>-0.027532</td>\n",
       "      <td>-0.024767</td>\n",
       "      <td>-0.020660</td>\n",
       "      <td>0.026382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.017873</td>\n",
       "      <td>0.017461</td>\n",
       "      <td>0.066402</td>\n",
       "      <td>0.081965</td>\n",
       "      <td>0.201064</td>\n",
       "      <td>0.411968</td>\n",
       "      <td>0.571916</td>\n",
       "      <td>0.679765</td>\n",
       "      <td>0.729753</td>\n",
       "      <td>0.765233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011958</td>\n",
       "      <td>0.016897</td>\n",
       "      <td>0.007306</td>\n",
       "      <td>0.017486</td>\n",
       "      <td>0.015705</td>\n",
       "      <td>-0.007856</td>\n",
       "      <td>-0.027394</td>\n",
       "      <td>-0.024784</td>\n",
       "      <td>-0.020647</td>\n",
       "      <td>-0.004744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.020970</td>\n",
       "      <td>0.014660</td>\n",
       "      <td>0.051292</td>\n",
       "      <td>0.060044</td>\n",
       "      <td>0.171395</td>\n",
       "      <td>0.383524</td>\n",
       "      <td>0.543309</td>\n",
       "      <td>0.645493</td>\n",
       "      <td>0.699912</td>\n",
       "      <td>0.731057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012760</td>\n",
       "      <td>0.017550</td>\n",
       "      <td>0.008633</td>\n",
       "      <td>0.018473</td>\n",
       "      <td>0.016209</td>\n",
       "      <td>-0.007599</td>\n",
       "      <td>-0.026772</td>\n",
       "      <td>-0.024282</td>\n",
       "      <td>-0.020383</td>\n",
       "      <td>-0.032617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.019335</td>\n",
       "      <td>0.012864</td>\n",
       "      <td>0.042932</td>\n",
       "      <td>0.038568</td>\n",
       "      <td>0.148259</td>\n",
       "      <td>0.355526</td>\n",
       "      <td>0.514403</td>\n",
       "      <td>0.613969</td>\n",
       "      <td>0.662203</td>\n",
       "      <td>0.697732</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013691</td>\n",
       "      <td>0.018297</td>\n",
       "      <td>0.009857</td>\n",
       "      <td>0.018888</td>\n",
       "      <td>0.016624</td>\n",
       "      <td>-0.006480</td>\n",
       "      <td>-0.025115</td>\n",
       "      <td>-0.022978</td>\n",
       "      <td>-0.019346</td>\n",
       "      <td>-0.060151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>-0.001629</td>\n",
       "      <td>0.057338</td>\n",
       "      <td>0.033023</td>\n",
       "      <td>-0.022975</td>\n",
       "      <td>-0.057054</td>\n",
       "      <td>-0.042952</td>\n",
       "      <td>-0.012753</td>\n",
       "      <td>0.013994</td>\n",
       "      <td>0.036818</td>\n",
       "      <td>0.061345</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269931</td>\n",
       "      <td>0.268204</td>\n",
       "      <td>0.237947</td>\n",
       "      <td>0.240019</td>\n",
       "      <td>0.233115</td>\n",
       "      <td>0.192533</td>\n",
       "      <td>0.185197</td>\n",
       "      <td>0.178767</td>\n",
       "      <td>0.165605</td>\n",
       "      <td>-0.066179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>-0.000670</td>\n",
       "      <td>0.053066</td>\n",
       "      <td>0.034183</td>\n",
       "      <td>-0.015997</td>\n",
       "      <td>-0.051284</td>\n",
       "      <td>-0.041445</td>\n",
       "      <td>-0.011790</td>\n",
       "      <td>0.015254</td>\n",
       "      <td>0.040054</td>\n",
       "      <td>0.066337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277196</td>\n",
       "      <td>0.274281</td>\n",
       "      <td>0.232947</td>\n",
       "      <td>0.232977</td>\n",
       "      <td>0.226130</td>\n",
       "      <td>0.176680</td>\n",
       "      <td>0.161686</td>\n",
       "      <td>0.153881</td>\n",
       "      <td>0.147017</td>\n",
       "      <td>-0.055425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>-0.001674</td>\n",
       "      <td>0.045043</td>\n",
       "      <td>0.031552</td>\n",
       "      <td>-0.010440</td>\n",
       "      <td>-0.044226</td>\n",
       "      <td>-0.040121</td>\n",
       "      <td>-0.013359</td>\n",
       "      <td>0.012053</td>\n",
       "      <td>0.037003</td>\n",
       "      <td>0.065395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.293001</td>\n",
       "      <td>0.289052</td>\n",
       "      <td>0.239432</td>\n",
       "      <td>0.238283</td>\n",
       "      <td>0.230021</td>\n",
       "      <td>0.175217</td>\n",
       "      <td>0.147287</td>\n",
       "      <td>0.138746</td>\n",
       "      <td>0.134543</td>\n",
       "      <td>-0.043645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>-0.002042</td>\n",
       "      <td>0.039683</td>\n",
       "      <td>0.027217</td>\n",
       "      <td>-0.008693</td>\n",
       "      <td>-0.040761</td>\n",
       "      <td>-0.038822</td>\n",
       "      <td>-0.013458</td>\n",
       "      <td>0.010409</td>\n",
       "      <td>0.035902</td>\n",
       "      <td>0.064555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.307212</td>\n",
       "      <td>0.302431</td>\n",
       "      <td>0.252937</td>\n",
       "      <td>0.249398</td>\n",
       "      <td>0.241787</td>\n",
       "      <td>0.185161</td>\n",
       "      <td>0.144957</td>\n",
       "      <td>0.138716</td>\n",
       "      <td>0.133271</td>\n",
       "      <td>-0.035670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.001637</td>\n",
       "      <td>0.034399</td>\n",
       "      <td>0.019538</td>\n",
       "      <td>-0.018879</td>\n",
       "      <td>-0.054686</td>\n",
       "      <td>-0.055442</td>\n",
       "      <td>-0.028789</td>\n",
       "      <td>-0.004473</td>\n",
       "      <td>0.022507</td>\n",
       "      <td>0.054078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.328993</td>\n",
       "      <td>0.322795</td>\n",
       "      <td>0.271715</td>\n",
       "      <td>0.263135</td>\n",
       "      <td>0.255623</td>\n",
       "      <td>0.196565</td>\n",
       "      <td>0.147686</td>\n",
       "      <td>0.142768</td>\n",
       "      <td>0.131620</td>\n",
       "      <td>-0.023495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>-0.000532</td>\n",
       "      <td>0.031616</td>\n",
       "      <td>0.018231</td>\n",
       "      <td>-0.015779</td>\n",
       "      <td>-0.048079</td>\n",
       "      <td>-0.048247</td>\n",
       "      <td>-0.021532</td>\n",
       "      <td>0.003280</td>\n",
       "      <td>0.032028</td>\n",
       "      <td>0.064938</td>\n",
       "      <td>...</td>\n",
       "      <td>0.343995</td>\n",
       "      <td>0.336245</td>\n",
       "      <td>0.281458</td>\n",
       "      <td>0.273021</td>\n",
       "      <td>0.264898</td>\n",
       "      <td>0.205694</td>\n",
       "      <td>0.151675</td>\n",
       "      <td>0.147324</td>\n",
       "      <td>0.131550</td>\n",
       "      <td>-0.012295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.030430</td>\n",
       "      <td>0.017610</td>\n",
       "      <td>-0.016601</td>\n",
       "      <td>-0.049213</td>\n",
       "      <td>-0.050212</td>\n",
       "      <td>-0.023386</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>0.030423</td>\n",
       "      <td>0.063434</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340095</td>\n",
       "      <td>0.331092</td>\n",
       "      <td>0.275081</td>\n",
       "      <td>0.267506</td>\n",
       "      <td>0.258708</td>\n",
       "      <td>0.200525</td>\n",
       "      <td>0.145584</td>\n",
       "      <td>0.141730</td>\n",
       "      <td>0.123685</td>\n",
       "      <td>-0.012726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.002648</td>\n",
       "      <td>0.023419</td>\n",
       "      <td>0.016116</td>\n",
       "      <td>-0.014337</td>\n",
       "      <td>-0.041753</td>\n",
       "      <td>-0.044956</td>\n",
       "      <td>-0.021274</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.031656</td>\n",
       "      <td>0.064791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.355309</td>\n",
       "      <td>0.345973</td>\n",
       "      <td>0.287815</td>\n",
       "      <td>0.277579</td>\n",
       "      <td>0.268672</td>\n",
       "      <td>0.207290</td>\n",
       "      <td>0.149051</td>\n",
       "      <td>0.144771</td>\n",
       "      <td>0.125076</td>\n",
       "      <td>-0.002546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0.003409</td>\n",
       "      <td>0.015330</td>\n",
       "      <td>0.014149</td>\n",
       "      <td>-0.009665</td>\n",
       "      <td>-0.036259</td>\n",
       "      <td>-0.042558</td>\n",
       "      <td>-0.021386</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.030364</td>\n",
       "      <td>0.064370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.368703</td>\n",
       "      <td>0.358869</td>\n",
       "      <td>0.299002</td>\n",
       "      <td>0.287229</td>\n",
       "      <td>0.278135</td>\n",
       "      <td>0.214202</td>\n",
       "      <td>0.152567</td>\n",
       "      <td>0.147951</td>\n",
       "      <td>0.126825</td>\n",
       "      <td>0.006195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0.001960</td>\n",
       "      <td>0.010891</td>\n",
       "      <td>0.012587</td>\n",
       "      <td>-0.008706</td>\n",
       "      <td>-0.033235</td>\n",
       "      <td>-0.038703</td>\n",
       "      <td>-0.017964</td>\n",
       "      <td>0.003437</td>\n",
       "      <td>0.034147</td>\n",
       "      <td>0.069713</td>\n",
       "      <td>...</td>\n",
       "      <td>0.377529</td>\n",
       "      <td>0.366419</td>\n",
       "      <td>0.304025</td>\n",
       "      <td>0.293009</td>\n",
       "      <td>0.283359</td>\n",
       "      <td>0.219662</td>\n",
       "      <td>0.155754</td>\n",
       "      <td>0.150219</td>\n",
       "      <td>0.128307</td>\n",
       "      <td>0.009966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0.002857</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>-0.006478</td>\n",
       "      <td>-0.028175</td>\n",
       "      <td>-0.038725</td>\n",
       "      <td>-0.019876</td>\n",
       "      <td>-0.000347</td>\n",
       "      <td>0.031010</td>\n",
       "      <td>0.067521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.393217</td>\n",
       "      <td>0.380662</td>\n",
       "      <td>0.314699</td>\n",
       "      <td>0.306227</td>\n",
       "      <td>0.296000</td>\n",
       "      <td>0.230179</td>\n",
       "      <td>0.164029</td>\n",
       "      <td>0.157591</td>\n",
       "      <td>0.134797</td>\n",
       "      <td>0.023007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0.000737</td>\n",
       "      <td>-0.000763</td>\n",
       "      <td>0.005798</td>\n",
       "      <td>-0.004687</td>\n",
       "      <td>-0.023782</td>\n",
       "      <td>-0.034336</td>\n",
       "      <td>-0.016779</td>\n",
       "      <td>0.002096</td>\n",
       "      <td>0.034011</td>\n",
       "      <td>0.071086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.404399</td>\n",
       "      <td>0.390959</td>\n",
       "      <td>0.321724</td>\n",
       "      <td>0.312559</td>\n",
       "      <td>0.302176</td>\n",
       "      <td>0.233733</td>\n",
       "      <td>0.164683</td>\n",
       "      <td>0.157475</td>\n",
       "      <td>0.133971</td>\n",
       "      <td>0.024388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0.001007</td>\n",
       "      <td>-0.001194</td>\n",
       "      <td>0.004683</td>\n",
       "      <td>-0.005196</td>\n",
       "      <td>-0.023970</td>\n",
       "      <td>-0.034313</td>\n",
       "      <td>-0.017281</td>\n",
       "      <td>0.001565</td>\n",
       "      <td>0.033015</td>\n",
       "      <td>0.069935</td>\n",
       "      <td>...</td>\n",
       "      <td>0.404811</td>\n",
       "      <td>0.390804</td>\n",
       "      <td>0.321626</td>\n",
       "      <td>0.312317</td>\n",
       "      <td>0.302277</td>\n",
       "      <td>0.233826</td>\n",
       "      <td>0.164996</td>\n",
       "      <td>0.157273</td>\n",
       "      <td>0.134275</td>\n",
       "      <td>0.022126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>-0.000600</td>\n",
       "      <td>0.009889</td>\n",
       "      <td>0.012315</td>\n",
       "      <td>0.002849</td>\n",
       "      <td>-0.016458</td>\n",
       "      <td>-0.028705</td>\n",
       "      <td>-0.013638</td>\n",
       "      <td>0.004509</td>\n",
       "      <td>0.037329</td>\n",
       "      <td>0.075295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.427362</td>\n",
       "      <td>0.413070</td>\n",
       "      <td>0.339036</td>\n",
       "      <td>0.327848</td>\n",
       "      <td>0.317145</td>\n",
       "      <td>0.246509</td>\n",
       "      <td>0.174234</td>\n",
       "      <td>0.165446</td>\n",
       "      <td>0.140868</td>\n",
       "      <td>0.027101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.013082</td>\n",
       "      <td>0.015886</td>\n",
       "      <td>0.007355</td>\n",
       "      <td>-0.013986</td>\n",
       "      <td>-0.027391</td>\n",
       "      <td>-0.013864</td>\n",
       "      <td>0.002766</td>\n",
       "      <td>0.035194</td>\n",
       "      <td>0.074310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.445081</td>\n",
       "      <td>0.430287</td>\n",
       "      <td>0.352167</td>\n",
       "      <td>0.337923</td>\n",
       "      <td>0.326995</td>\n",
       "      <td>0.254503</td>\n",
       "      <td>0.178624</td>\n",
       "      <td>0.168569</td>\n",
       "      <td>0.142264</td>\n",
       "      <td>0.027753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>0.007992</td>\n",
       "      <td>0.001646</td>\n",
       "      <td>-0.010592</td>\n",
       "      <td>-0.016188</td>\n",
       "      <td>-0.004070</td>\n",
       "      <td>0.008674</td>\n",
       "      <td>0.034834</td>\n",
       "      <td>0.065641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.501349</td>\n",
       "      <td>0.483742</td>\n",
       "      <td>0.395176</td>\n",
       "      <td>0.377731</td>\n",
       "      <td>0.365423</td>\n",
       "      <td>0.284478</td>\n",
       "      <td>0.198447</td>\n",
       "      <td>0.186364</td>\n",
       "      <td>0.155842</td>\n",
       "      <td>0.019562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0.001476</td>\n",
       "      <td>-0.007399</td>\n",
       "      <td>-0.001338</td>\n",
       "      <td>-0.007308</td>\n",
       "      <td>0.001064</td>\n",
       "      <td>0.008428</td>\n",
       "      <td>0.018340</td>\n",
       "      <td>0.024046</td>\n",
       "      <td>0.035996</td>\n",
       "      <td>0.052080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.604108</td>\n",
       "      <td>0.583380</td>\n",
       "      <td>0.476721</td>\n",
       "      <td>0.452457</td>\n",
       "      <td>0.437708</td>\n",
       "      <td>0.342021</td>\n",
       "      <td>0.240101</td>\n",
       "      <td>0.224506</td>\n",
       "      <td>0.189391</td>\n",
       "      <td>0.007947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0.004502</td>\n",
       "      <td>-0.023752</td>\n",
       "      <td>-0.014690</td>\n",
       "      <td>-0.013845</td>\n",
       "      <td>0.007598</td>\n",
       "      <td>0.017654</td>\n",
       "      <td>0.024445</td>\n",
       "      <td>0.025552</td>\n",
       "      <td>0.029452</td>\n",
       "      <td>0.036055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.757757</td>\n",
       "      <td>0.730042</td>\n",
       "      <td>0.597509</td>\n",
       "      <td>0.563626</td>\n",
       "      <td>0.544431</td>\n",
       "      <td>0.425705</td>\n",
       "      <td>0.303201</td>\n",
       "      <td>0.282201</td>\n",
       "      <td>0.238304</td>\n",
       "      <td>-0.003511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0.003046</td>\n",
       "      <td>-0.021839</td>\n",
       "      <td>-0.011278</td>\n",
       "      <td>-0.010976</td>\n",
       "      <td>0.009717</td>\n",
       "      <td>0.020145</td>\n",
       "      <td>0.026494</td>\n",
       "      <td>0.027341</td>\n",
       "      <td>0.031251</td>\n",
       "      <td>0.037875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.780555</td>\n",
       "      <td>0.752718</td>\n",
       "      <td>0.614670</td>\n",
       "      <td>0.577087</td>\n",
       "      <td>0.556631</td>\n",
       "      <td>0.436582</td>\n",
       "      <td>0.312840</td>\n",
       "      <td>0.290745</td>\n",
       "      <td>0.246647</td>\n",
       "      <td>-0.004741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0.006798</td>\n",
       "      <td>-0.030800</td>\n",
       "      <td>-0.019892</td>\n",
       "      <td>-0.017085</td>\n",
       "      <td>0.014868</td>\n",
       "      <td>0.033823</td>\n",
       "      <td>0.040549</td>\n",
       "      <td>0.039532</td>\n",
       "      <td>0.038435</td>\n",
       "      <td>0.038314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.864341</td>\n",
       "      <td>0.832781</td>\n",
       "      <td>0.681383</td>\n",
       "      <td>0.634521</td>\n",
       "      <td>0.611680</td>\n",
       "      <td>0.482674</td>\n",
       "      <td>0.349374</td>\n",
       "      <td>0.323736</td>\n",
       "      <td>0.275724</td>\n",
       "      <td>-0.007655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.000459</td>\n",
       "      <td>-0.014121</td>\n",
       "      <td>-0.008986</td>\n",
       "      <td>-0.010596</td>\n",
       "      <td>0.010476</td>\n",
       "      <td>0.021815</td>\n",
       "      <td>0.024732</td>\n",
       "      <td>0.022336</td>\n",
       "      <td>0.020624</td>\n",
       "      <td>0.020235</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962304</td>\n",
       "      <td>0.790342</td>\n",
       "      <td>0.732212</td>\n",
       "      <td>0.703087</td>\n",
       "      <td>0.560425</td>\n",
       "      <td>0.412233</td>\n",
       "      <td>0.381236</td>\n",
       "      <td>0.326997</td>\n",
       "      <td>-0.020590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.001774</td>\n",
       "      <td>-0.016809</td>\n",
       "      <td>-0.009755</td>\n",
       "      <td>-0.009218</td>\n",
       "      <td>0.013435</td>\n",
       "      <td>0.025169</td>\n",
       "      <td>0.028833</td>\n",
       "      <td>0.027147</td>\n",
       "      <td>0.025515</td>\n",
       "      <td>0.025054</td>\n",
       "      <td>...</td>\n",
       "      <td>0.962304</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.827642</td>\n",
       "      <td>0.762129</td>\n",
       "      <td>0.729660</td>\n",
       "      <td>0.589927</td>\n",
       "      <td>0.440696</td>\n",
       "      <td>0.407512</td>\n",
       "      <td>0.353494</td>\n",
       "      <td>-0.015425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>-0.004780</td>\n",
       "      <td>-0.015669</td>\n",
       "      <td>-0.002319</td>\n",
       "      <td>-0.000767</td>\n",
       "      <td>0.014569</td>\n",
       "      <td>0.017714</td>\n",
       "      <td>0.018541</td>\n",
       "      <td>0.015354</td>\n",
       "      <td>0.013352</td>\n",
       "      <td>0.012868</td>\n",
       "      <td>...</td>\n",
       "      <td>0.790342</td>\n",
       "      <td>0.827642</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.915262</td>\n",
       "      <td>0.880142</td>\n",
       "      <td>0.708903</td>\n",
       "      <td>0.543124</td>\n",
       "      <td>0.503272</td>\n",
       "      <td>0.434893</td>\n",
       "      <td>-0.020142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>-0.004405</td>\n",
       "      <td>-0.012065</td>\n",
       "      <td>0.004868</td>\n",
       "      <td>0.008452</td>\n",
       "      <td>0.025945</td>\n",
       "      <td>0.030095</td>\n",
       "      <td>0.030198</td>\n",
       "      <td>0.027008</td>\n",
       "      <td>0.025011</td>\n",
       "      <td>0.024328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.732212</td>\n",
       "      <td>0.762129</td>\n",
       "      <td>0.915262</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.960526</td>\n",
       "      <td>0.774880</td>\n",
       "      <td>0.601604</td>\n",
       "      <td>0.560280</td>\n",
       "      <td>0.486545</td>\n",
       "      <td>-0.010170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>-0.007503</td>\n",
       "      <td>-0.008725</td>\n",
       "      <td>0.009894</td>\n",
       "      <td>0.010660</td>\n",
       "      <td>0.027586</td>\n",
       "      <td>0.031137</td>\n",
       "      <td>0.030683</td>\n",
       "      <td>0.027375</td>\n",
       "      <td>0.025351</td>\n",
       "      <td>0.024391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.703087</td>\n",
       "      <td>0.729660</td>\n",
       "      <td>0.880142</td>\n",
       "      <td>0.960526</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.801899</td>\n",
       "      <td>0.635195</td>\n",
       "      <td>0.593602</td>\n",
       "      <td>0.512015</td>\n",
       "      <td>-0.005861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>-0.005206</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>0.002109</td>\n",
       "      <td>0.003815</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.006153</td>\n",
       "      <td>0.007837</td>\n",
       "      <td>0.007368</td>\n",
       "      <td>0.005706</td>\n",
       "      <td>0.005428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560425</td>\n",
       "      <td>0.589927</td>\n",
       "      <td>0.708903</td>\n",
       "      <td>0.774880</td>\n",
       "      <td>0.801899</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.798790</td>\n",
       "      <td>0.747729</td>\n",
       "      <td>0.641034</td>\n",
       "      <td>-0.014477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>-0.001655</td>\n",
       "      <td>-0.002080</td>\n",
       "      <td>0.006442</td>\n",
       "      <td>0.013094</td>\n",
       "      <td>0.003762</td>\n",
       "      <td>-0.005582</td>\n",
       "      <td>-0.009596</td>\n",
       "      <td>-0.012403</td>\n",
       "      <td>-0.014867</td>\n",
       "      <td>-0.015574</td>\n",
       "      <td>...</td>\n",
       "      <td>0.412233</td>\n",
       "      <td>0.440696</td>\n",
       "      <td>0.543124</td>\n",
       "      <td>0.601604</td>\n",
       "      <td>0.635195</td>\n",
       "      <td>0.798790</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.939798</td>\n",
       "      <td>0.806979</td>\n",
       "      <td>-0.020374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0.001748</td>\n",
       "      <td>-0.005430</td>\n",
       "      <td>0.004487</td>\n",
       "      <td>0.013047</td>\n",
       "      <td>0.006255</td>\n",
       "      <td>-0.001191</td>\n",
       "      <td>-0.005762</td>\n",
       "      <td>-0.009182</td>\n",
       "      <td>-0.011476</td>\n",
       "      <td>-0.012136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.381236</td>\n",
       "      <td>0.407512</td>\n",
       "      <td>0.503272</td>\n",
       "      <td>0.560280</td>\n",
       "      <td>0.593602</td>\n",
       "      <td>0.747729</td>\n",
       "      <td>0.939798</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.843296</td>\n",
       "      <td>-0.014618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0.000728</td>\n",
       "      <td>-0.013417</td>\n",
       "      <td>-0.002900</td>\n",
       "      <td>0.013421</td>\n",
       "      <td>0.011019</td>\n",
       "      <td>0.002726</td>\n",
       "      <td>-0.002896</td>\n",
       "      <td>-0.007000</td>\n",
       "      <td>-0.009371</td>\n",
       "      <td>-0.010147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.326997</td>\n",
       "      <td>0.353494</td>\n",
       "      <td>0.434893</td>\n",
       "      <td>0.486545</td>\n",
       "      <td>0.512015</td>\n",
       "      <td>0.641034</td>\n",
       "      <td>0.806979</td>\n",
       "      <td>0.843296</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.021185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>-0.054520</td>\n",
       "      <td>0.032274</td>\n",
       "      <td>0.227131</td>\n",
       "      <td>0.306713</td>\n",
       "      <td>0.337507</td>\n",
       "      <td>0.276126</td>\n",
       "      <td>0.213227</td>\n",
       "      <td>0.206502</td>\n",
       "      <td>0.208454</td>\n",
       "      <td>0.207081</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020590</td>\n",
       "      <td>-0.015425</td>\n",
       "      <td>-0.020142</td>\n",
       "      <td>-0.010170</td>\n",
       "      <td>-0.005861</td>\n",
       "      <td>-0.014477</td>\n",
       "      <td>-0.020374</td>\n",
       "      <td>-0.014618</td>\n",
       "      <td>-0.021185</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>185 rows × 185 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6  \\\n",
       "0       1.000000 -0.144666 -0.177616 -0.152790 -0.096287 -0.065342 -0.055155   \n",
       "1      -0.144666  1.000000  0.751248  0.500707  0.305988  0.212290  0.181973   \n",
       "2      -0.177616  0.751248  1.000000  0.838487  0.622316  0.472374  0.384922   \n",
       "3      -0.152790  0.500707  0.838487  1.000000  0.872682  0.688740  0.561711   \n",
       "4      -0.096287  0.305988  0.622316  0.872682  1.000000  0.894447  0.763733   \n",
       "5      -0.065342  0.212290  0.472374  0.688740  0.894447  1.000000  0.939087   \n",
       "6      -0.055155  0.181973  0.384922  0.561711  0.763733  0.939087  1.000000   \n",
       "7      -0.046589  0.167057  0.339727  0.479327  0.661905  0.851245  0.956465   \n",
       "8      -0.041210  0.159117  0.316231  0.443540  0.602007  0.786819  0.908970   \n",
       "9      -0.038315  0.155542  0.304816  0.422033  0.568914  0.741474  0.871848   \n",
       "10     -0.036839  0.145552  0.290853  0.403009  0.540335  0.713634  0.841678   \n",
       "11     -0.031709  0.143267  0.278851  0.388595  0.525051  0.693748  0.827019   \n",
       "12     -0.030020  0.134683  0.269779  0.373314  0.507993  0.678131  0.807888   \n",
       "13     -0.028928  0.133368  0.260469  0.363856  0.493537  0.664019  0.795842   \n",
       "14     -0.026765  0.127772  0.252131  0.347797  0.479941  0.646577  0.781524   \n",
       "15     -0.026061  0.118841  0.241677  0.333328  0.460753  0.632024  0.763219   \n",
       "16     -0.019610  0.115867  0.229069  0.317812  0.445454  0.613594  0.751415   \n",
       "17     -0.015755  0.103024  0.216203  0.298890  0.426767  0.598883  0.736706   \n",
       "18     -0.015344  0.097122  0.202663  0.286897  0.413629  0.589740  0.730408   \n",
       "19     -0.009646  0.083423  0.179758  0.253454  0.382197  0.557571  0.702003   \n",
       "20     -0.006830  0.059098  0.147493  0.215405  0.340937  0.526901  0.674260   \n",
       "21     -0.001725  0.044249  0.119768  0.182764  0.307545  0.493882  0.651316   \n",
       "22      0.005993  0.033550  0.107875  0.162072  0.289274  0.483738  0.640591   \n",
       "23      0.010304  0.039312  0.109502  0.166528  0.292913  0.490359  0.648534   \n",
       "24      0.011958  0.040149  0.109621  0.157950  0.288612  0.484417  0.643996   \n",
       "25      0.011408  0.036453  0.103524  0.144557  0.270292  0.474470  0.630004   \n",
       "26      0.014238  0.032773  0.088128  0.119389  0.242694  0.447163  0.609046   \n",
       "27      0.017873  0.017461  0.066402  0.081965  0.201064  0.411968  0.571916   \n",
       "28      0.020970  0.014660  0.051292  0.060044  0.171395  0.383524  0.543309   \n",
       "29      0.019335  0.012864  0.042932  0.038568  0.148259  0.355526  0.514403   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "155    -0.001629  0.057338  0.033023 -0.022975 -0.057054 -0.042952 -0.012753   \n",
       "156    -0.000670  0.053066  0.034183 -0.015997 -0.051284 -0.041445 -0.011790   \n",
       "157    -0.001674  0.045043  0.031552 -0.010440 -0.044226 -0.040121 -0.013359   \n",
       "158    -0.002042  0.039683  0.027217 -0.008693 -0.040761 -0.038822 -0.013458   \n",
       "159     0.001637  0.034399  0.019538 -0.018879 -0.054686 -0.055442 -0.028789   \n",
       "160    -0.000532  0.031616  0.018231 -0.015779 -0.048079 -0.048247 -0.021532   \n",
       "161     0.000085  0.030430  0.017610 -0.016601 -0.049213 -0.050212 -0.023386   \n",
       "162     0.002648  0.023419  0.016116 -0.014337 -0.041753 -0.044956 -0.021274   \n",
       "163     0.003409  0.015330  0.014149 -0.009665 -0.036259 -0.042558 -0.021386   \n",
       "164     0.001960  0.010891  0.012587 -0.008706 -0.033235 -0.038703 -0.017964   \n",
       "165     0.002857  0.001036  0.007143 -0.006478 -0.028175 -0.038725 -0.019876   \n",
       "166     0.000737 -0.000763  0.005798 -0.004687 -0.023782 -0.034336 -0.016779   \n",
       "167     0.001007 -0.001194  0.004683 -0.005196 -0.023970 -0.034313 -0.017281   \n",
       "168    -0.000600  0.009889  0.012315  0.002849 -0.016458 -0.028705 -0.013638   \n",
       "169     0.000101  0.013082  0.015886  0.007355 -0.013986 -0.027391 -0.013864   \n",
       "170     0.000800  0.003394  0.007992  0.001646 -0.010592 -0.016188 -0.004070   \n",
       "171     0.001476 -0.007399 -0.001338 -0.007308  0.001064  0.008428  0.018340   \n",
       "172     0.004502 -0.023752 -0.014690 -0.013845  0.007598  0.017654  0.024445   \n",
       "173     0.003046 -0.021839 -0.011278 -0.010976  0.009717  0.020145  0.026494   \n",
       "174     0.006798 -0.030800 -0.019892 -0.017085  0.014868  0.033823  0.040549   \n",
       "175     0.000459 -0.014121 -0.008986 -0.010596  0.010476  0.021815  0.024732   \n",
       "176     0.001774 -0.016809 -0.009755 -0.009218  0.013435  0.025169  0.028833   \n",
       "177    -0.004780 -0.015669 -0.002319 -0.000767  0.014569  0.017714  0.018541   \n",
       "178    -0.004405 -0.012065  0.004868  0.008452  0.025945  0.030095  0.030198   \n",
       "179    -0.007503 -0.008725  0.009894  0.010660  0.027586  0.031137  0.030683   \n",
       "180    -0.005206  0.000359  0.002109  0.003815  0.000372  0.006153  0.007837   \n",
       "181    -0.001655 -0.002080  0.006442  0.013094  0.003762 -0.005582 -0.009596   \n",
       "182     0.001748 -0.005430  0.004487  0.013047  0.006255 -0.001191 -0.005762   \n",
       "183     0.000728 -0.013417 -0.002900  0.013421  0.011019  0.002726 -0.002896   \n",
       "target -0.054520  0.032274  0.227131  0.306713  0.337507  0.276126  0.213227   \n",
       "\n",
       "               7         8         9    ...          175       176       177  \\\n",
       "0      -0.046589 -0.041210 -0.038315    ...     0.000459  0.001774 -0.004780   \n",
       "1       0.167057  0.159117  0.155542    ...    -0.014121 -0.016809 -0.015669   \n",
       "2       0.339727  0.316231  0.304816    ...    -0.008986 -0.009755 -0.002319   \n",
       "3       0.479327  0.443540  0.422033    ...    -0.010596 -0.009218 -0.000767   \n",
       "4       0.661905  0.602007  0.568914    ...     0.010476  0.013435  0.014569   \n",
       "5       0.851245  0.786819  0.741474    ...     0.021815  0.025169  0.017714   \n",
       "6       0.956465  0.908970  0.871848    ...     0.024732  0.028833  0.018541   \n",
       "7       1.000000  0.976515  0.952481    ...     0.022336  0.027147  0.015354   \n",
       "8       0.976515  1.000000  0.984051    ...     0.020624  0.025515  0.013352   \n",
       "9       0.952481  0.984051  1.000000    ...     0.020235  0.025054  0.012868   \n",
       "10      0.930747  0.970819  0.988453    ...     0.020492  0.024999  0.012814   \n",
       "11      0.911671  0.956062  0.978919    ...     0.020620  0.024562  0.012498   \n",
       "12      0.899652  0.941352  0.968868    ...     0.021041  0.024721  0.012598   \n",
       "13      0.886751  0.935541  0.959676    ...     0.018600  0.022531  0.011268   \n",
       "14      0.875866  0.921680  0.953206    ...     0.019154  0.023352  0.011549   \n",
       "15      0.861346  0.909323  0.936118    ...     0.018870  0.022965  0.011934   \n",
       "16      0.846490  0.896696  0.925324    ...     0.018589  0.022758  0.011972   \n",
       "17      0.840162  0.887786  0.919132    ...     0.018408  0.022460  0.011437   \n",
       "18      0.830623  0.885410  0.913458    ...     0.017291  0.021505  0.010749   \n",
       "19      0.805509  0.858494  0.893039    ...     0.017421  0.021651  0.010842   \n",
       "20      0.783439  0.839609  0.871788    ...     0.016782  0.020970  0.010469   \n",
       "21      0.758381  0.816703  0.851037    ...     0.017358  0.021906  0.011710   \n",
       "22      0.755335  0.809926  0.846735    ...     0.017582  0.022081  0.011581   \n",
       "23      0.757380  0.816135  0.847791    ...     0.014523  0.019052  0.009495   \n",
       "24      0.751993  0.804470  0.840643    ...     0.013778  0.018455  0.008540   \n",
       "25      0.737695  0.790637  0.821950    ...     0.011876  0.016556  0.007012   \n",
       "26      0.711606  0.765740  0.798631    ...     0.011107  0.016048  0.006552   \n",
       "27      0.679765  0.729753  0.765233    ...     0.011958  0.016897  0.007306   \n",
       "28      0.645493  0.699912  0.731057    ...     0.012760  0.017550  0.008633   \n",
       "29      0.613969  0.662203  0.697732    ...     0.013691  0.018297  0.009857   \n",
       "...          ...       ...       ...    ...          ...       ...       ...   \n",
       "155     0.013994  0.036818  0.061345    ...     0.269931  0.268204  0.237947   \n",
       "156     0.015254  0.040054  0.066337    ...     0.277196  0.274281  0.232947   \n",
       "157     0.012053  0.037003  0.065395    ...     0.293001  0.289052  0.239432   \n",
       "158     0.010409  0.035902  0.064555    ...     0.307212  0.302431  0.252937   \n",
       "159    -0.004473  0.022507  0.054078    ...     0.328993  0.322795  0.271715   \n",
       "160     0.003280  0.032028  0.064938    ...     0.343995  0.336245  0.281458   \n",
       "161     0.001394  0.030423  0.063434    ...     0.340095  0.331092  0.275081   \n",
       "162     0.002976  0.031656  0.064791    ...     0.355309  0.345973  0.287815   \n",
       "163     0.000651  0.030364  0.064370    ...     0.368703  0.358869  0.299002   \n",
       "164     0.003437  0.034147  0.069713    ...     0.377529  0.366419  0.304025   \n",
       "165    -0.000347  0.031010  0.067521    ...     0.393217  0.380662  0.314699   \n",
       "166     0.002096  0.034011  0.071086    ...     0.404399  0.390959  0.321724   \n",
       "167     0.001565  0.033015  0.069935    ...     0.404811  0.390804  0.321626   \n",
       "168     0.004509  0.037329  0.075295    ...     0.427362  0.413070  0.339036   \n",
       "169     0.002766  0.035194  0.074310    ...     0.445081  0.430287  0.352167   \n",
       "170     0.008674  0.034834  0.065641    ...     0.501349  0.483742  0.395176   \n",
       "171     0.024046  0.035996  0.052080    ...     0.604108  0.583380  0.476721   \n",
       "172     0.025552  0.029452  0.036055    ...     0.757757  0.730042  0.597509   \n",
       "173     0.027341  0.031251  0.037875    ...     0.780555  0.752718  0.614670   \n",
       "174     0.039532  0.038435  0.038314    ...     0.864341  0.832781  0.681383   \n",
       "175     0.022336  0.020624  0.020235    ...     1.000000  0.962304  0.790342   \n",
       "176     0.027147  0.025515  0.025054    ...     0.962304  1.000000  0.827642   \n",
       "177     0.015354  0.013352  0.012868    ...     0.790342  0.827642  1.000000   \n",
       "178     0.027008  0.025011  0.024328    ...     0.732212  0.762129  0.915262   \n",
       "179     0.027375  0.025351  0.024391    ...     0.703087  0.729660  0.880142   \n",
       "180     0.007368  0.005706  0.005428    ...     0.560425  0.589927  0.708903   \n",
       "181    -0.012403 -0.014867 -0.015574    ...     0.412233  0.440696  0.543124   \n",
       "182    -0.009182 -0.011476 -0.012136    ...     0.381236  0.407512  0.503272   \n",
       "183    -0.007000 -0.009371 -0.010147    ...     0.326997  0.353494  0.434893   \n",
       "target  0.206502  0.208454  0.207081    ...    -0.020590 -0.015425 -0.020142   \n",
       "\n",
       "             178       179       180       181       182       183    target  \n",
       "0      -0.004405 -0.007503 -0.005206 -0.001655  0.001748  0.000728 -0.054520  \n",
       "1      -0.012065 -0.008725  0.000359 -0.002080 -0.005430 -0.013417  0.032274  \n",
       "2       0.004868  0.009894  0.002109  0.006442  0.004487 -0.002900  0.227131  \n",
       "3       0.008452  0.010660  0.003815  0.013094  0.013047  0.013421  0.306713  \n",
       "4       0.025945  0.027586  0.000372  0.003762  0.006255  0.011019  0.337507  \n",
       "5       0.030095  0.031137  0.006153 -0.005582 -0.001191  0.002726  0.276126  \n",
       "6       0.030198  0.030683  0.007837 -0.009596 -0.005762 -0.002896  0.213227  \n",
       "7       0.027008  0.027375  0.007368 -0.012403 -0.009182 -0.007000  0.206502  \n",
       "8       0.025011  0.025351  0.005706 -0.014867 -0.011476 -0.009371  0.208454  \n",
       "9       0.024328  0.024391  0.005428 -0.015574 -0.012136 -0.010147  0.207081  \n",
       "10      0.024350  0.024494  0.005562 -0.015985 -0.012582 -0.010825  0.205424  \n",
       "11      0.023476  0.023792  0.004970 -0.016622 -0.013275 -0.011175  0.200058  \n",
       "12      0.023616  0.023584  0.004151 -0.016935 -0.013592 -0.011809  0.197330  \n",
       "13      0.022034  0.022100  0.003690 -0.017140 -0.013808 -0.011621  0.194571  \n",
       "14      0.022185  0.022558  0.004120 -0.017522 -0.014228 -0.012073  0.190590  \n",
       "15      0.022611  0.022803  0.003039 -0.018248 -0.014935 -0.012982  0.187275  \n",
       "16      0.022472  0.022416  0.002553 -0.018148 -0.014934 -0.012468  0.179537  \n",
       "17      0.022042  0.022074  0.002529 -0.018745 -0.015619 -0.013056  0.173814  \n",
       "18      0.021282  0.021348  0.001593 -0.019166 -0.016138 -0.013312  0.166952  \n",
       "19      0.021292  0.020912  0.000886 -0.020178 -0.017193 -0.014309  0.154225  \n",
       "20      0.021113  0.020927  0.000204 -0.020913 -0.017847 -0.014712  0.140680  \n",
       "21      0.022277  0.021932  0.000400 -0.020701 -0.017716 -0.014310  0.122510  \n",
       "22      0.022328  0.021891 -0.000383 -0.021794 -0.018873 -0.015396  0.108900  \n",
       "23      0.020326  0.019503 -0.002940 -0.023969 -0.021007 -0.017389  0.095446  \n",
       "24      0.019384  0.018258 -0.005007 -0.025427 -0.022516 -0.018662  0.076036  \n",
       "25      0.017811  0.016775 -0.005949 -0.025981 -0.023102 -0.019127  0.054966  \n",
       "26      0.016982  0.015484 -0.008185 -0.027532 -0.024767 -0.020660  0.026382  \n",
       "27      0.017486  0.015705 -0.007856 -0.027394 -0.024784 -0.020647 -0.004744  \n",
       "28      0.018473  0.016209 -0.007599 -0.026772 -0.024282 -0.020383 -0.032617  \n",
       "29      0.018888  0.016624 -0.006480 -0.025115 -0.022978 -0.019346 -0.060151  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "155     0.240019  0.233115  0.192533  0.185197  0.178767  0.165605 -0.066179  \n",
       "156     0.232977  0.226130  0.176680  0.161686  0.153881  0.147017 -0.055425  \n",
       "157     0.238283  0.230021  0.175217  0.147287  0.138746  0.134543 -0.043645  \n",
       "158     0.249398  0.241787  0.185161  0.144957  0.138716  0.133271 -0.035670  \n",
       "159     0.263135  0.255623  0.196565  0.147686  0.142768  0.131620 -0.023495  \n",
       "160     0.273021  0.264898  0.205694  0.151675  0.147324  0.131550 -0.012295  \n",
       "161     0.267506  0.258708  0.200525  0.145584  0.141730  0.123685 -0.012726  \n",
       "162     0.277579  0.268672  0.207290  0.149051  0.144771  0.125076 -0.002546  \n",
       "163     0.287229  0.278135  0.214202  0.152567  0.147951  0.126825  0.006195  \n",
       "164     0.293009  0.283359  0.219662  0.155754  0.150219  0.128307  0.009966  \n",
       "165     0.306227  0.296000  0.230179  0.164029  0.157591  0.134797  0.023007  \n",
       "166     0.312559  0.302176  0.233733  0.164683  0.157475  0.133971  0.024388  \n",
       "167     0.312317  0.302277  0.233826  0.164996  0.157273  0.134275  0.022126  \n",
       "168     0.327848  0.317145  0.246509  0.174234  0.165446  0.140868  0.027101  \n",
       "169     0.337923  0.326995  0.254503  0.178624  0.168569  0.142264  0.027753  \n",
       "170     0.377731  0.365423  0.284478  0.198447  0.186364  0.155842  0.019562  \n",
       "171     0.452457  0.437708  0.342021  0.240101  0.224506  0.189391  0.007947  \n",
       "172     0.563626  0.544431  0.425705  0.303201  0.282201  0.238304 -0.003511  \n",
       "173     0.577087  0.556631  0.436582  0.312840  0.290745  0.246647 -0.004741  \n",
       "174     0.634521  0.611680  0.482674  0.349374  0.323736  0.275724 -0.007655  \n",
       "175     0.732212  0.703087  0.560425  0.412233  0.381236  0.326997 -0.020590  \n",
       "176     0.762129  0.729660  0.589927  0.440696  0.407512  0.353494 -0.015425  \n",
       "177     0.915262  0.880142  0.708903  0.543124  0.503272  0.434893 -0.020142  \n",
       "178     1.000000  0.960526  0.774880  0.601604  0.560280  0.486545 -0.010170  \n",
       "179     0.960526  1.000000  0.801899  0.635195  0.593602  0.512015 -0.005861  \n",
       "180     0.774880  0.801899  1.000000  0.798790  0.747729  0.641034 -0.014477  \n",
       "181     0.601604  0.635195  0.798790  1.000000  0.939798  0.806979 -0.020374  \n",
       "182     0.560280  0.593602  0.747729  0.939798  1.000000  0.843296 -0.014618  \n",
       "183     0.486545  0.512015  0.641034  0.806979  0.843296  1.000000 -0.021185  \n",
       "target -0.010170 -0.005861 -0.014477 -0.020374 -0.014618 -0.021185  1.000000  \n",
       "\n",
       "[185 rows x 185 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(columns=[184,185,186,187])\n",
    "data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.c Regresión Logistica\n",
    "Realice una primera regresión logística con los datos. Que valor entrega el metodo `score`?\n",
    "\n",
    "Calcule la matriz de confusión y comente respecto al valor dado por el `score`. Comente sobre la distinta información entregrada por la matriz de confusión y su interpretación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LogisticRegression()\n",
    "# . . \n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "confusion_matrix = model_selection.confusion_matrix(real_labels, predicted_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.d Otra métrica. \n",
    "Otra forma de comparar modelos podría ser con la siguiente métrica. Esta mide la cantidad de aciertos pero toma en cuenta algo más. Que interpretación podría darle a la métrica en el código? \n",
    "\n",
    "Si un modelo entrega 0.5 y otro 0.4 en esta métrica, cual es mejor? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((np.max(reg.predict_proba(x_val),axis=1)*(reg.predict(x_val)-y_val))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.e PCA y LDA\n",
    "Transforme los datos mediante PCA a 2 dimensiones y grafique un subsampleo de alrededor de 50 - 100 datos con 50% de cada clase. Coloree cada punto correspondiente a la clase a la que pertenece. \n",
    "\n",
    "Realice el mismo analisis para LDA, considerando el mismo conjunto de puntos. Cual de ambos métodos le parece más apropiado?\n",
    "\n",
    "Grafíque el centro de gravedad (promedio en cada cordenada) de cada una de las clases luego de la reducción de dimensionalidad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import \n",
    "pca = decomposition.PCA(n_components=2)\n",
    "X_class1_pca = pca.fit_transform(X_class_1,y_class_1)\n",
    "# . . . \n",
    "\n",
    "from sklearn import discriminant_analysis \n",
    "lda = discriminant_analysis.LinearDiscriminantAnalysis(n_components=2)\n",
    "# . . .   . . .   . . . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.f Más reducción de dimensionalidad\n",
    "Entrene un modelo de regresión logística para el método que le pareció más apropiado de la pregunta aterior, haciendo variar el numero de componentes de 2 a 120. Calcule para cada modelo alguna métrica de la calidad del modelo tanto sobre el conjunto de entrenamiento como sobre el conjunto de validación. \n",
    "\n",
    "Grafíque como varían ambos errores en función de la dimensionalidad sobre la que se trabaja. \n",
    "\n",
    "Si le sobra *tiempo* y _c r e a t i v i d a d_  proponga cambios o extienda la exploración buscando mejorar el desempeño o comprender mejor el problema (solo para puntaje extra).\n",
    "\n",
    "[//]: <> (  https://meme.xyz/uploads/posts/t/l-27712-i-procrastinate-to-the-last-minute-den-i-freaked-it.jpg )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.g Naive Bayes\n",
    "\"Entrene\" un modelo utilizando Naive Bayes y comparelo con el modelo logístico que entrenó anteriormente. Le parece adecuado el nombre _Naive_?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes as sknb\n",
    "NB = sknb.BernoulliNB()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
